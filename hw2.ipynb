{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class BasicNNModule:\n",
    "    def __init__(self):\n",
    "        self.lr = 0.001\n",
    "\n",
    "class SingleLinear(BasicNNModule):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.weights = np.ones((out_features, in_features)) * 0.5\n",
    "        self.bias = np.zeros(out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return np.matmul(x, self.weights.T) + self.bias\n",
    "    \n",
    "    def backward(self, delta_next_layer, w_next_layer, b_next_layer, a_cur_layer, y_last_layer):\n",
    "        # print(d2y.shape, y2i.shape, y_last_layer.shape)\n",
    "        # delta = d2y * y2i\n",
    "        delta = (np.matmul(delta_next_layer.T, w_next_layer) * a_cur_layer).T\n",
    "        # print(delta)\n",
    "        # L2 regularization\n",
    "        self.weights += self.lr * (np.matmul(delta, y_last_layer) - 0.01 * self.weights)\n",
    "        # self.weights += self.lr * np.matmul(delta, y_last_layer)\n",
    "        self.bias += self.lr * delta.T[0]\n",
    "        return delta, self.weights, self.bias\n",
    "    \n",
    "    def softmax_backward(self, y, label, y_last_layer):\n",
    "        delta = (y - label).T\n",
    "        # print('before update:', self.weights, self.bias)\n",
    "        # print(delta.shape, y_last_layer.shape, self.bias.shape)\n",
    "\n",
    "        # L2 regularization\n",
    "        self.weights += self.lr * (np.matmul(delta, y_last_layer) - 0.01 * self.weights)\n",
    "        # self.weights += self.lr * np.matmul(delta, y_last_layer)\n",
    "\n",
    "        self.bias += self.lr * delta.T[0]\n",
    "        # print('after update:', self.weights, self.bias)\n",
    "        return delta, self.weights, self.bias\n",
    "\n",
    "class ReLU:\n",
    "    def forward(self, x):\n",
    "        return np.maximum(x, 0)\n",
    "    \n",
    "    def backward(self, x):\n",
    "        return np.where(x > 0, 1, 0)\n",
    "    \n",
    "class Softmax:\n",
    "    def forward(self, x):\n",
    "        # 減去最大值以防止 overflow\n",
    "        x = x - np.max(x, axis=1, keepdims=True)\n",
    "        exp_x = np.exp(x)\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "with open('Iris.csv') as f:\n",
    "    data = f.readlines()[1:]\n",
    "\n",
    "data = [line.strip().split(',')[1:] for line in data]\n",
    "data = np.array(data)\n",
    "labels = []\n",
    "\n",
    "classes = np.unique(data[:, 4])\n",
    "\n",
    "for d in data:\n",
    "    for cls in classes:\n",
    "        if d[4] == cls:\n",
    "            one_hot = np.zeros(len(classes))\n",
    "            one_hot[classes.tolist().index(cls)] = 1\n",
    "            labels.append(one_hot)\n",
    "\n",
    "inputs = data[:, :4].astype(np.float32)\n",
    "\n",
    "# inputs = (inputs - np.mean(inputs, axis=0)) / np.std(inputs, axis=0)\n",
    "\n",
    "labels = np.array(labels)\n",
    "\n",
    "# 打亂數據\n",
    "idx = np.random.permutation(len(inputs))\n",
    "inputs = inputs[idx]\n",
    "labels = labels[idx]\n",
    "\n",
    "\n",
    "linear_1 = SingleLinear(4, 5)\n",
    "relu_1 = ReLU()\n",
    "linear_2 = SingleLinear(5, 5)\n",
    "relu_2 = ReLU()\n",
    "linear_3 = SingleLinear(5, 3)\n",
    "softmax_3 = Softmax()\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    for input, label in zip(inputs, labels):\n",
    "        input = input.reshape(1, -1)\n",
    "        # print(input, label)\n",
    "        i_1 = linear_1.forward(input)\n",
    "        y_1 = relu_1.forward(i_1)\n",
    "        i_2 = linear_2.forward(y_1)\n",
    "        y_2 = relu_2.forward(i_2)\n",
    "        i_3 = linear_3.forward(y_2)\n",
    "        print(\"i_3\", i_3)\n",
    "        y_3 = softmax_3.forward(i_3)\n",
    "        print(\"y_3\", y_3)\n",
    "        print(\"label\", label)\n",
    "        delta_3, w_3, b_3 = linear_3.softmax_backward(y_3, label, y_2)\n",
    "        delta_2, w_2, b_2 = linear_2.backward(delta_3, w_3, b_3, relu_2.backward(i_2), y_1)\n",
    "        delta_1, w_1, b_1 = linear_1.backward(delta_2, w_2, b_2, relu_1.backward(i_1), input)\n",
    "        # print('delta_3:\\n', delta_3)\n",
    "        # print('w_3:\\n', w_3)\n",
    "        # print('b_3:\\n', b_3)\n",
    "        \n",
    "        # print('delta_2:\\n', delta_2)\n",
    "        # print('w_2:\\n', w_2)\n",
    "        # print('b_2:\\n', b_2)\n",
    "        # break\n",
    "        # delta_1 = linear_1.backward(- ())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(delta_3.T, w_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01714783, 0.93623955, 0.04661262]])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Softmax:\n",
    "    def forward(self, x):\n",
    "        # 減去最大值以防止 overflow\n",
    "        x = x - np.max(x, axis=1, keepdims=True)\n",
    "        exp_x = np.exp(x)\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "Softmax().forward(np.array([[1, 5, 2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.95799297  0.50648367 -1.29696496  0.99630143]\n",
      " [-0.48203369  0.28668749  2.12959588  0.65556684]\n",
      " [-0.08816886  0.089359   -0.41808245  0.38364675]]\n",
      "[ 0.09246073  0.36246263  0.09009849 -0.19246959]\n",
      "[-0.21360764  0.12504099 -0.08727179]\n"
     ]
    }
   ],
   "source": [
    "a = np.random.randn(3, 4)\n",
    "b = np.random.randn(4)\n",
    "print(a)\n",
    "print(b)\n",
    "print(a.dot(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 150), (3, 150))"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "with open('Iris.csv') as f:\n",
    "    data = f.readlines()[1:]\n",
    "\n",
    "data = [line.strip().split(',')[1:] for line in data]\n",
    "data = np.array(data)\n",
    "labels = []\n",
    "\n",
    "classes = np.unique(data[:, 4])\n",
    "\n",
    "for d in data:\n",
    "    for cls in classes:\n",
    "        if d[4] == cls:\n",
    "            one_hot = np.zeros(len(classes))\n",
    "            one_hot[classes.tolist().index(cls)] = 1\n",
    "            labels.append(one_hot)\n",
    "\n",
    "inputs = data[:, :4].astype(np.float32)\n",
    "\n",
    "# inputs = (inputs - np.mean(inputs, axis=0)) / np.std(inputs, axis=0)\n",
    "\n",
    "labels = np.array(labels)\n",
    "\n",
    "# 打亂數據\n",
    "idx = np.random.permutation(len(inputs))\n",
    "inputs = inputs[idx].T\n",
    "labels = labels[idx].T\n",
    "\n",
    "\n",
    "inputs.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m: 150 n: 4\n"
     ]
    }
   ],
   "source": [
    "m, n = inputs.T.shape\n",
    "\n",
    "print('m:', m, 'n:', n)\n",
    "\n",
    "def init_params():\n",
    "    W1 = np.random.rand(10, 4) - 0.5\n",
    "    b1 = np.random.rand(10, 1) - 0.5\n",
    "    W2 = np.random.rand(3, 10) - 0.5\n",
    "    b2 = np.random.rand(3, 1) - 0.5\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "def ReLU(Z):\n",
    "    return np.maximum(Z, 0)\n",
    "\n",
    "def ReLU_deriv(Z):\n",
    "    return Z > 0\n",
    "\n",
    "def softmax(Z):\n",
    "    A = np.exp(Z) / sum(np.exp(Z))\n",
    "    return A\n",
    "    \n",
    "def forward_prop(W1, b1, W2, b2, X):\n",
    "    # print('X shape:', X.shape, 'W1 shape:', W1.shape)\n",
    "    Z1 = np.matmul(W1, X) + b1\n",
    "    A1 = ReLU(Z1)\n",
    "    Z2 = np.matmul(W2, A1) + b2\n",
    "    A2 = softmax(Z2)\n",
    "    return Z1, A1, Z2, A2\n",
    "\n",
    "\n",
    "\n",
    "def one_hot(Y):\n",
    "    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "    one_hot_Y = one_hot_Y.T\n",
    "    return one_hot_Y\n",
    "\n",
    "def backward_prop(Z1, A1, A2, W2, X, Y):\n",
    "    one_hot_Y = one_hot(Y)\n",
    "    print('A2 shape:', A2.shape, 'Y shape:', Y.shape, 'one_hot_Y shape:', one_hot_Y.shape)\n",
    "    dZ2 = A2 - one_hot_Y\n",
    "    # dW2 = 1 / m * dZ2.dot(A1.T)\n",
    "    # print('dZ2 shape:', dZ2.shape, 'A1 shape:', A1.shape)\n",
    "    dW2 = (1 / m) * np.matmul(dZ2, A1.T)\n",
    "    # print(dW2.shape)\n",
    "    db2 = 1 / m * np.sum(dZ2)\n",
    "    dZ1 = W2.T.dot(dZ2) * ReLU_deriv(Z1)\n",
    "    # dW1 = 1 / m * dZ1.dot(X.T)\n",
    "    dW1 = (1 / m) * np.matmul(dZ1, X.T)\n",
    "    db1 = 1 / m * np.sum(dZ1)\n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n",
    "    W1 = W1 - alpha * dW1\n",
    "    b1 = b1 - alpha * db1    \n",
    "    W2 = W2 - alpha * dW2  \n",
    "    b2 = b2 - alpha * db2    \n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "def get_predictions(A2):\n",
    "    return np.argmax(A2, 0)\n",
    "\n",
    "def get_accuracy(predictions, Y):\n",
    "    # print(predictions, Y)\n",
    "    return np.sum(predictions == Y) / Y.size\n",
    "\n",
    "def gradient_descent(X, Y, alpha, iterations):\n",
    "    W1, b1, W2, b2 = init_params()\n",
    "    for i in range(iterations):\n",
    "        # for X, Y in zip(X_all.T, Y_all.T):\n",
    "        #     X = X.reshape(-1, 1)\n",
    "        #     Y = Y.reshape(-1, 1)\n",
    "            # print('Y shape:', Y.shape, 'Y_all shape:', Y_all.shape)\n",
    "            # print('X_all shape:', X_all.shape)\n",
    "            # print('X shape:', X.shape)\n",
    "        Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X)\n",
    "        dW1, db1, dW2, db2 = backward_prop(Z1, A1, A2, W2, X, Y)\n",
    "        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
    "        if i % 10 == 0:\n",
    "            print(\"Iteration: \", i)\n",
    "\n",
    "            predictions = get_predictions(A2)\n",
    "            print(get_accuracy(predictions, Y))\n",
    "        break\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A2 shape: (3, 150) Y shape: (150,) one_hot_Y shape: (3, 150)\n",
      "Iteration:  0\n",
      "0.31333333333333335\n"
     ]
    }
   ],
   "source": [
    "X_train = inputs\n",
    "Y_train = np.argmax(labels, axis=0)\n",
    "# Y_train = labels\n",
    "W1, b1, W2, b2 = gradient_descent(X_train, Y_train, 0.10, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m: 150 n: 4\n"
     ]
    }
   ],
   "source": [
    "m, n = inputs.T.shape\n",
    "\n",
    "print('m:', m, 'n:', n)\n",
    "\n",
    "def init_params():\n",
    "    W1 = np.random.rand(10, 4) - 0.5\n",
    "    b1 = np.random.rand(10, 1) - 0.5\n",
    "    W2 = np.random.rand(3, 10) - 0.5\n",
    "    b2 = np.random.rand(3, 1) - 0.5\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "def ReLU(Z):\n",
    "    return np.maximum(Z, 0)\n",
    "\n",
    "def ReLU_deriv(Z):\n",
    "    return Z > 0\n",
    "\n",
    "def softmax(Z):\n",
    "    A = np.exp(Z) / sum(np.exp(Z))\n",
    "    return A\n",
    "    \n",
    "def forward_prop(W1, b1, W2, b2, X):\n",
    "    # print('X shape:', X.shape, 'W1 shape:', W1.shape)\n",
    "    Z1 = np.matmul(W1, X) + b1\n",
    "    A1 = ReLU(Z1)\n",
    "    Z2 = np.matmul(W2, A1) + b2\n",
    "    A2 = softmax(Z2)\n",
    "    return Z1, A1, Z2, A2\n",
    "\n",
    "\n",
    "\n",
    "def one_hot(Y):\n",
    "    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "    one_hot_Y = one_hot_Y.T\n",
    "    return one_hot_Y\n",
    "\n",
    "def backward_prop(Z1, A1, A2, W2, X, Y):\n",
    "    # one_hot_Y = Y.reshape(-1, 1)\n",
    "    one_hot_Y = one_hot(Y)\n",
    "    # print('A2 shape:', A2.shape, 'Y shape:', Y.shape, 'one_hot_Y shape:', one_hot_Y.shape)\n",
    "    dZ2 = A2 - one_hot_Y\n",
    "    # dW2 = 1 / m * dZ2.dot(A1.T)\n",
    "    # print('dZ2 shape:', dZ2.shape, 'A1 shape:', A1.shape)\n",
    "    dW2 = (1 / m) * np.matmul(dZ2, A1.T)\n",
    "    # print(dW2.shape)\n",
    "    db2 = (1 / m) * np.sum(dZ2)\n",
    "    dZ1 = W2.T.dot(dZ2) * ReLU_deriv(Z1)\n",
    "    # dW1 = 1 / m * dZ1.dot(X.T)\n",
    "    dW1 = (1 / m) * np.matmul(dZ1, X.T)\n",
    "    db1 = 1 / m * np.sum(dZ1)\n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n",
    "    W1 = W1 - alpha * dW1\n",
    "    b1 = b1 - alpha * db1    \n",
    "    W2 = W2 - alpha * dW2  \n",
    "    b2 = b2 - alpha * db2    \n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "def get_predictions(A2):\n",
    "    return np.argmax(A2, 0)\n",
    "\n",
    "def get_accuracy(predictions, Y):\n",
    "    # print(predictions, Y)\n",
    "    return np.sum(predictions == Y) / Y.size\n",
    "\n",
    "def gradient_descent(X, Y, alpha, iterations):\n",
    "    bs = 10\n",
    "    W1, b1, W2, b2 = init_params()\n",
    "    for i in range(iterations):\n",
    "        # for X, Y_s in zip(X_all.T, Y_all.T):\n",
    "\n",
    "        #     X = X.reshape(-1, 1)\n",
    "        #     Y = Y.reshape(-1, 1)\n",
    "        # print('Y shape:', Y.shape, 'Y_all shape:', Y_all.shape)\n",
    "        # print('X_all shape:', X_all.shape)\n",
    "        # print('X shape:', X.shape)\n",
    "        Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X)\n",
    "        dW1, db1, dW2, db2 = backward_prop(Z1, A1, A2, W2, X, Y)\n",
    "        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
    "        if i % 50 == 0:\n",
    "            print(\"Iteration: \", i)\n",
    "\n",
    "            predictions = get_predictions(A2)\n",
    "            print(get_accuracy(predictions, Y))\n",
    "        # break\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "0.3333333333333333\n",
      "Iteration:  50\n",
      "0.9133333333333333\n",
      "Iteration:  100\n",
      "0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "X_train = inputs\n",
    "Y_train = np.argmax(labels, axis=0)\n",
    "# Y_train = labels\n",
    "W1, b1, W2, b2 = gradient_descent(X_train, Y_train, 0.05, 101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ======================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params():\n",
    "    # W1 = np.random.rand(4, 10) - 0.5  # Changed from (10, 4) to (4, 10)\n",
    "    # b1 = np.random.rand(1, 10) - 0.5  # Changed from (10, 1) to (1, 10)\n",
    "    # W2 = np.random.rand(10, 3) - 0.5  # Changed from (3, 10) to (10, 3)\n",
    "    # b2 = np.random.rand(1, 3) - 0.5   # Changed from (3, 1) to (1, 3)\n",
    "    W1 = np.ones((4, 10))\n",
    "    b1 = np.zeros((1, 10))\n",
    "    W2 = np.ones((10, 3))\n",
    "    b2 = np.zeros((1, 3))\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "def ReLU(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "def d_ReLU(x):\n",
    "    return x > 0\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "def forward_prop(W1, b1, W2, b2, X):\n",
    "    # X shape: (n_samples, n_features)\n",
    "    I1 = np.matmul(X, W1) + b1  # Changed matrix multiplication order\n",
    "    Y1 = ReLU(I1)\n",
    "    I2 = np.matmul(Y1, W2) + b2  # Changed matrix multiplication order\n",
    "    Y2 = softmax(I2)\n",
    "    return I1, Y1, I2, Y2\n",
    "\n",
    "def backward_prop(I1, Y1, Y2, W2, X, Y):\n",
    "\n",
    "    m = X.shape[0]\n",
    "    norm_factor = 1 / m\n",
    "    \n",
    "    dZ2 = Y2 - Y  # (n_samples, 3)\n",
    "    # print('dZ2 shape:', dZ2.shape, 'A1.t shape:', A1.T.shape)\n",
    "    dW2 = norm_factor * np.matmul(Y1.T, dZ2)  # Changed matrix multiplication order\n",
    "    db2 = norm_factor * np.sum(dZ2, axis=0, keepdims=True)\n",
    "    \n",
    "    dZ1 = np.matmul(dZ2, W2.T) * d_ReLU(I1)  # Changed matrix multiplication order\n",
    "    dW1 = norm_factor * np.matmul(X.T, dZ1)  # Changed matrix multiplication order\n",
    "    db1 = norm_factor * np.sum(dZ1, axis=0, keepdims=True)\n",
    "    \n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n",
    "    W1 = W1 - alpha * dW1\n",
    "    b1 = b1 - alpha * db1\n",
    "    W2 = W2 - alpha * dW2\n",
    "    b2 = b2 - alpha * db2\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "def get_predictions(A2):\n",
    "    return np.argmax(A2, axis=1)\n",
    "\n",
    "def get_accuracy(predictions, Y):\n",
    "    Y = np.argmax(Y, axis=1)\n",
    "    return np.sum(predictions == Y) / Y.size\n",
    "\n",
    "def gradient_descent(X_all, Y_all, X_val, Y_val, alpha, iterations, bs):\n",
    "    # X_all -> (n_samples, n_features)\n",
    "    # Y_all -> (n_samples, n_classes) one-hot \n",
    "    \n",
    "    W1, b1, W2, b2 = init_params()\n",
    "    \n",
    "    X_batch = X_all.reshape(-1, bs, X_all.shape[1])\n",
    "    Y_batch = Y_all.reshape(-1, bs, Y_all.shape[1])\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        loss = 0\n",
    "        for X, Y in zip(X_batch, Y_batch):\n",
    "            # X = X.reshape(1, -1)\n",
    "            # Y = Y.reshape(1, -1)\n",
    "            # print('Y shape:', Y.shape, 'Y_all shape:', Y_all.shape)\n",
    "            I1, Y1, I2, Y2 = forward_prop(W1, b1, W2, b2, X)\n",
    "            dW1, db1, dW2, db2 = backward_prop(I1, Y1, Y2, W2, X, Y)\n",
    "            W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
    "            loss += -np.sum(Y * np.log(Y2))\n",
    "\n",
    "        if i % 1 == 0:\n",
    "            print(\"Iteration: \", i)\n",
    "            print(\"Loss: \", loss)\n",
    "            _, _, _, pred = forward_prop(W1, b1, W2, b2, X_val)\n",
    "            predictions = get_predictions(pred)\n",
    "            print('Acc:', get_accuracy(predictions, Y_val))\n",
    "            # print(predictions, Y)\n",
    "        # break\n",
    "\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 4)\n",
      "Iteration:  0\n",
      "Loss:  1001.7608705156697\n",
      "Acc: 0.6583333333333333\n",
      "Iteration:  1\n",
      "Loss:  90.47267922908023\n",
      "Acc: 0.6916666666666667\n",
      "Iteration:  2\n",
      "Loss:  75.40924592445778\n",
      "Acc: 0.6916666666666667\n",
      "Iteration:  3\n",
      "Loss:  67.86493349199213\n",
      "Acc: 0.7333333333333333\n",
      "Iteration:  4\n",
      "Loss:  62.505519460259904\n",
      "Acc: 0.7583333333333333\n",
      "Iteration:  5\n",
      "Loss:  58.39690512425387\n",
      "Acc: 0.775\n",
      "Iteration:  6\n",
      "Loss:  55.08818587889734\n",
      "Acc: 0.8083333333333333\n",
      "Iteration:  7\n",
      "Loss:  52.32543400820918\n",
      "Acc: 0.825\n",
      "Iteration:  8\n",
      "Loss:  49.95495163481795\n",
      "Acc: 0.85\n",
      "Iteration:  9\n",
      "Loss:  47.878584861316476\n",
      "Acc: 0.875\n",
      "Iteration:  10\n",
      "Loss:  46.030678749272724\n",
      "Acc: 0.875\n",
      "Iteration:  11\n",
      "Loss:  44.36547332794938\n",
      "Acc: 0.8916666666666667\n",
      "Iteration:  12\n",
      "Loss:  42.84989433520057\n",
      "Acc: 0.8916666666666667\n",
      "Iteration:  13\n",
      "Loss:  41.459270293248174\n",
      "Acc: 0.925\n",
      "Iteration:  14\n",
      "Loss:  40.17469989727743\n",
      "Acc: 0.925\n",
      "Iteration:  15\n",
      "Loss:  38.98138057768009\n",
      "Acc: 0.9416666666666667\n",
      "Iteration:  16\n",
      "Loss:  37.86751291851595\n",
      "Acc: 0.9583333333333334\n",
      "Iteration:  17\n",
      "Loss:  36.8235593955592\n",
      "Acc: 0.9583333333333334\n",
      "Iteration:  18\n",
      "Loss:  35.84172694506643\n",
      "Acc: 0.9583333333333334\n",
      "Iteration:  19\n",
      "Loss:  34.915594697685634\n",
      "Acc: 0.9666666666666667\n",
      "Iteration:  20\n",
      "Loss:  34.039838307162405\n",
      "Acc: 0.9666666666666667\n",
      "Iteration:  21\n",
      "Loss:  33.2100201295346\n",
      "Acc: 0.9666666666666667\n",
      "Iteration:  22\n",
      "Loss:  32.42242529909555\n",
      "Acc: 0.9666666666666667\n",
      "Iteration:  23\n",
      "Loss:  31.673930446534143\n",
      "Acc: 0.9666666666666667\n",
      "Iteration:  24\n",
      "Loss:  30.96189609116575\n",
      "Acc: 0.9666666666666667\n",
      "Iteration:  25\n",
      "Loss:  30.28407658162235\n",
      "Acc: 0.9583333333333334\n",
      "Iteration:  26\n",
      "Loss:  29.638543419957468\n",
      "Acc: 0.9583333333333334\n",
      "Iteration:  27\n",
      "Loss:  29.02361920690523\n",
      "Acc: 0.9666666666666667\n",
      "Iteration:  28\n",
      "Loss:  28.437820472344708\n",
      "Acc: 0.9666666666666667\n",
      "Iteration:  29\n",
      "Loss:  27.879808400359668\n",
      "Acc: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 固定隨機種子，確保同參數下每次結果都相同\n",
    "np.random.seed(1)\n",
    "\n",
    "with open('Iris.csv') as f:\n",
    "    data = f.readlines()[1:]\n",
    "\n",
    "data = [line.strip().split(',')[1:] for line in data]\n",
    "data = np.array(data)\n",
    "labels = []\n",
    "\n",
    "classes = np.unique(data[:, 4])\n",
    "\n",
    "for d in data:\n",
    "    for cls in classes:\n",
    "        if d[4] == cls:\n",
    "            one_hot = np.zeros(len(classes))\n",
    "            one_hot[classes.tolist().index(cls)] = 1\n",
    "            labels.append(one_hot)\n",
    "\n",
    "inputs = data[:, :4].astype(np.float32)\n",
    "\n",
    "# inputs = (inputs - np.mean(inputs, axis=0)) / np.std(inputs, axis=0)\n",
    "\n",
    "labels = np.array(labels)\n",
    "\n",
    "# 打亂數據\n",
    "idx = np.random.permutation(len(inputs))\n",
    "\n",
    "X_data = inputs[idx]\n",
    "# Y_train = np.argmax(labels, axis=0)\n",
    "\n",
    "Y_data = labels[idx]\n",
    "\n",
    "train_size = int(len(X_data) * 0.8)\n",
    "val_size = len(X_data) - train_size\n",
    "\n",
    "X_train, Y_train = X_data[:train_size], Y_data[:train_size]\n",
    "X_val, Y_val = X_data[val_size:], Y_data[val_size:]\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "batch_size = 3\n",
    "lr = 0.05\n",
    "\n",
    "W1, b1, W2, b2 = gradient_descent(X_train, Y_train, X_val, Y_val, lr, 30, batch_size)\n",
    "# W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 7.9566765e-01, -1.2495735e-01,  9.9022174e-01,  7.9059029e-01],\n",
       "        [ 6.7449981e-01,  3.3784863e-01,  8.7649041e-01,  1.4479550e+00]],\n",
       "\n",
       "       [[ 3.1099588e-01, -3.5636008e-01,  5.3529590e-01,  2.6469851e-01],\n",
       "        [ 1.0380032e+00,  1.0644536e-01,  5.3529590e-01,  3.9617148e-01]],\n",
       "\n",
       "       [[ 7.9566765e-01, -1.2495735e-01,  8.1962448e-01,  1.0535363e+00],\n",
       "        [ 6.7449981e-01, -3.5636008e-01,  3.0783325e-01,  1.3322552e-01]],\n",
       "\n",
       "       [[ 4.3216369e-01, -3.5636008e-01,  3.0783325e-01,  1.3322552e-01],\n",
       "        [-2.9484364e-01, -3.5636008e-01, -9.0226904e-02,  1.3322552e-01]],\n",
       "\n",
       "       [[ 5.5333203e-01, -1.2819721e+00,  6.4902753e-01,  3.9617148e-01],\n",
       "        [-4.1601142e-01, -1.7447780e+00,  1.3723601e-01,  1.3322552e-01]],\n",
       "\n",
       "       [[-5.3717923e-01, -1.2495735e-01,  4.2156458e-01,  3.9617148e-01],\n",
       "        [-5.3717923e-01,  8.0065459e-01, -1.1706754e+00, -1.3129770e+00]],\n",
       "\n",
       "       [[ 4.3216369e-01, -5.8776331e-01,  5.9216183e-01,  7.9059029e-01],\n",
       "        [-1.5065227e+00,  3.3784863e-01, -1.3412726e+00, -1.3129770e+00]],\n",
       "\n",
       "       [[ 1.0380032e+00, -1.2495735e-01,  7.0589316e-01,  6.5911746e-01],\n",
       "        [-6.5834701e-01,  1.4948633e+00, -1.2844069e+00, -1.3129770e+00]],\n",
       "\n",
       "       [[ 6.7449981e-01,  3.3784863e-01,  4.2156458e-01,  3.9617148e-01],\n",
       "        [ 2.2496822e+00, -1.2495735e-01,  1.3314160e+00,  1.4479550e+00]],\n",
       "\n",
       "       [[ 1.5226749e+00, -1.2495735e-01,  1.2176846e+00,  1.1850091e+00],\n",
       "        [ 2.2496822e+00, -1.0505693e+00,  1.7863418e+00,  1.4479550e+00]],\n",
       "\n",
       "       [[-4.1601142e-01, -1.5133747e+00,  2.3504555e-02, -1.2972030e-01],\n",
       "        [-5.2507486e-02, -8.1916606e-01,  8.0370352e-02,  1.7526889e-03]],\n",
       "\n",
       "       [[-1.7488583e+00,  3.3784863e-01, -1.3981383e+00, -1.3129770e+00],\n",
       "        [ 1.0380032e+00, -1.2495735e-01,  8.1962448e-01,  1.4479550e+00]],\n",
       "\n",
       "       [[-7.7951539e-01,  2.4204748e+00, -1.2844069e+00, -1.4444498e+00],\n",
       "        [-4.1601142e-01, -1.2819721e+00,  1.3723601e-01,  1.3322552e-01]],\n",
       "\n",
       "       [[-1.7367585e-01, -5.8776331e-01,  4.2156458e-01,  1.3322552e-01],\n",
       "        [ 1.0380032e+00,  5.6925136e-01,  1.1039531e+00,  1.1850091e+00]],\n",
       "\n",
       "       [[-1.0218509e+00, -2.4389868e+00, -1.4709257e-01, -2.6119328e-01],\n",
       "        [ 6.8660304e-02, -1.2495735e-01,  7.6275885e-01,  7.9059029e-01]],\n",
       "\n",
       "       [[ 1.1591716e+00, -5.8776331e-01,  5.9216183e-01,  2.6469851e-01],\n",
       "        [ 5.5333203e-01, -5.8776331e-01,  7.6275885e-01,  3.9617148e-01]],\n",
       "\n",
       "       [[ 9.1683543e-01, -3.5636008e-01,  4.7843024e-01,  1.3322552e-01],\n",
       "        [-1.0218509e+00, -1.7447780e+00, -2.6082402e-01, -2.6119328e-01]],\n",
       "\n",
       "       [[-1.3853549e+00,  3.3784863e-01, -1.3981383e+00, -1.3129770e+00],\n",
       "        [ 9.1683543e-01, -1.2495735e-01,  3.6469892e-01,  2.6469851e-01]],\n",
       "\n",
       "       [[-1.0218509e+00,  1.0320573e+00, -1.3981383e+00, -1.1815039e+00],\n",
       "        [ 7.9566765e-01, -5.8776331e-01,  4.7843024e-01,  3.9617148e-01]],\n",
       "\n",
       "       [[-4.1601142e-01,  1.0320573e+00, -1.3981383e+00, -1.3129770e+00],\n",
       "        [ 6.7449981e-01, -5.8776331e-01,  1.0470874e+00,  1.1850091e+00]],\n",
       "\n",
       "       [[-1.7367585e-01, -1.2495735e-01,  2.5096732e-01,  1.7526889e-03],\n",
       "        [ 2.2496822e+00,  1.7262660e+00,  1.6726102e+00,  1.3164822e+00]],\n",
       "\n",
       "       [[ 5.5333203e-01, -3.5636008e-01,  1.0470874e+00,  7.9059029e-01],\n",
       "        [-5.2507486e-02, -8.1916606e-01,  7.6275885e-01,  9.2206323e-01]],\n",
       "\n",
       "       [[ 1.0380032e+00,  1.0644536e-01,  3.6469892e-01,  2.6469851e-01],\n",
       "        [ 1.8982810e-01, -1.9761807e+00,  7.0589316e-01,  3.9617148e-01]],\n",
       "\n",
       "       [[ 6.8660304e-02,  3.3784863e-01,  5.9216183e-01,  7.9059029e-01],\n",
       "        [ 6.7449981e-01,  1.0644536e-01,  9.9022174e-01,  7.9059029e-01]],\n",
       "\n",
       "       [[-1.7367585e-01, -1.0505693e+00, -1.4709257e-01, -2.6119328e-01],\n",
       "        [-1.1430187e+00,  1.0644536e-01, -1.2844069e+00, -1.4444498e+00]],\n",
       "\n",
       "       [[ 5.5333203e-01,  8.0065459e-01,  1.0470874e+00,  1.5794282e+00],\n",
       "        [-1.0218509e+00, -1.2495735e-01, -1.2275412e+00, -1.3129770e+00]],\n",
       "\n",
       "       [[ 6.7449981e-01, -5.8776331e-01,  1.0470874e+00,  1.3164822e+00],\n",
       "        [ 1.0380032e+00,  1.0644536e-01,  1.0470874e+00,  1.5794282e+00]],\n",
       "\n",
       "       [[-5.2507486e-02, -8.1916606e-01,  1.9410168e-01, -2.6119328e-01],\n",
       "        [-1.2641865e+00,  1.0644536e-01, -1.2275412e+00, -1.3129770e+00]],\n",
       "\n",
       "       [[ 7.9566765e-01,  3.3784863e-01,  7.6275885e-01,  1.0535363e+00],\n",
       "        [-1.5065227e+00,  1.0644536e-01, -1.2844069e+00, -1.3129770e+00]],\n",
       "\n",
       "       [[-1.7488583e+00, -3.5636008e-01, -1.3412726e+00, -1.3129770e+00],\n",
       "        [-1.2641865e+00, -1.2495735e-01, -1.3412726e+00, -1.1815039e+00]],\n",
       "\n",
       "       [[ 1.6438427e+00,  3.3784863e-01,  1.2745503e+00,  7.9059029e-01],\n",
       "        [-2.9484364e-01, -1.2819721e+00,  8.0370352e-02, -1.2972030e-01]],\n",
       "\n",
       "       [[-1.1430187e+00, -1.2495735e-01, -1.3412726e+00, -1.3129770e+00],\n",
       "        [ 5.5333203e-01, -1.2819721e+00,  7.0589316e-01,  9.2206323e-01]],\n",
       "\n",
       "       [[-5.3717923e-01,  1.4948633e+00, -1.2844069e+00, -1.3129770e+00],\n",
       "        [-7.7951539e-01,  8.0065459e-01, -1.3412726e+00, -1.3129770e+00]],\n",
       "\n",
       "       [[-5.2507486e-02, -1.0505693e+00,  1.3723601e-01,  1.7526889e-03],\n",
       "        [-1.0218509e+00,  3.3784863e-01, -1.4550040e+00, -1.3129770e+00]],\n",
       "\n",
       "       [[-5.3717923e-01,  1.9576693e+00, -1.1706754e+00, -1.0500311e+00],\n",
       "        [ 1.8982810e-01, -1.2495735e-01,  5.9216183e-01,  7.9059029e-01]],\n",
       "\n",
       "       [[-9.0068316e-01,  1.4948633e+00, -1.2844069e+00, -1.0500311e+00],\n",
       "        [ 3.1099588e-01, -1.0505693e+00,  1.0470874e+00,  2.6469851e-01]],\n",
       "\n",
       "       [[ 1.8982810e-01,  8.0065459e-01,  4.2156458e-01,  5.2764446e-01],\n",
       "        [ 6.7449981e-01, -8.1916606e-01,  8.7649041e-01,  9.2206323e-01]],\n",
       "\n",
       "       [[ 1.8982810e-01, -1.9761807e+00,  1.3723601e-01, -2.6119328e-01],\n",
       "        [ 4.3216369e-01,  8.0065459e-01,  9.3335611e-01,  1.4479550e+00]],\n",
       "\n",
       "       [[-2.9484364e-01, -1.2495735e-01,  1.9410168e-01,  1.3322552e-01],\n",
       "        [-1.0218509e+00,  1.2634600e+00, -1.3412726e+00, -1.3129770e+00]],\n",
       "\n",
       "       [[ 5.5333203e-01, -8.1916606e-01,  6.4902753e-01,  7.9059029e-01],\n",
       "        [-5.3717923e-01,  8.0065459e-01, -1.2844069e+00, -1.0500311e+00]],\n",
       "\n",
       "       [[ 1.4015071e+00,  3.3784863e-01,  5.3529590e-01,  2.6469851e-01],\n",
       "        [-9.0068316e-01,  1.0320573e+00, -1.3412726e+00, -1.1815039e+00]],\n",
       "\n",
       "       [[-1.8700261e+00, -1.2495735e-01, -1.5118698e+00, -1.4444498e+00],\n",
       "        [ 6.8660304e-02, -1.2495735e-01,  2.5096732e-01,  3.9617148e-01]],\n",
       "\n",
       "       [[ 3.1099588e-01, -1.2495735e-01,  4.7843024e-01,  2.6469851e-01],\n",
       "        [ 1.2803394e+00,  1.0644536e-01,  6.4902753e-01,  3.9617148e-01]],\n",
       "\n",
       "       [[-9.0068316e-01,  1.0320573e+00, -1.3412726e+00, -1.3129770e+00],\n",
       "        [-1.6276904e+00, -1.7447780e+00, -1.3981383e+00, -1.1815039e+00]],\n",
       "\n",
       "       [[-1.1430187e+00,  1.0644536e-01, -1.2844069e+00, -1.4444498e+00],\n",
       "        [-1.7488583e+00, -1.2495735e-01, -1.3981383e+00, -1.3129770e+00]],\n",
       "\n",
       "       [[-9.0068316e-01,  5.6925136e-01, -1.1706754e+00, -9.1855806e-01],\n",
       "        [ 5.5333203e-01,  5.6925136e-01,  1.2745503e+00,  1.7109010e+00]],\n",
       "\n",
       "       [[-1.5065227e+00,  1.2634600e+00, -1.5687355e+00, -1.3129770e+00],\n",
       "        [ 5.5333203e-01,  5.6925136e-01,  5.3529590e-01,  5.2764446e-01]],\n",
       "\n",
       "       [[ 1.2803394e+00,  3.3784863e-01,  1.1039531e+00,  1.4479550e+00],\n",
       "        [-5.3717923e-01,  1.9576693e+00, -1.3981383e+00, -1.0500311e+00]],\n",
       "\n",
       "       [[ 1.8861789e+00, -5.8776331e-01,  1.3314160e+00,  9.2206323e-01],\n",
       "        [-1.1430187e+00,  1.0644536e-01, -1.2844069e+00, -1.4444498e+00]],\n",
       "\n",
       "       [[-1.7367585e-01, -5.8776331e-01,  1.9410168e-01,  1.3322552e-01],\n",
       "        [ 1.1591716e+00, -1.2495735e-01,  9.9022174e-01,  1.1850091e+00]],\n",
       "\n",
       "       [[-7.7951539e-01,  1.0320573e+00, -1.2844069e+00, -1.3129770e+00],\n",
       "        [-4.1601142e-01, -1.5133747e+00, -3.3361107e-02, -2.6119328e-01]],\n",
       "\n",
       "       [[ 7.9566765e-01, -1.2495735e-01,  1.1608191e+00,  1.3164822e+00],\n",
       "        [-7.7951539e-01, -8.1916606e-01,  8.0370352e-02,  2.6469851e-01]],\n",
       "\n",
       "       [[-5.2507486e-02, -5.8776331e-01,  7.6275885e-01,  1.5794282e+00],\n",
       "        [-5.2507486e-02,  2.1890719e+00, -1.4550040e+00, -1.3129770e+00]],\n",
       "\n",
       "       [[ 2.1285145e+00, -1.2495735e-01,  1.6157446e+00,  1.1850091e+00],\n",
       "        [ 1.2803394e+00,  1.0644536e-01,  9.3335611e-01,  1.1850091e+00]],\n",
       "\n",
       "       [[-1.0218509e+00,  5.6925136e-01, -1.3412726e+00, -1.3129770e+00],\n",
       "        [ 1.8982810e-01, -8.1916606e-01,  7.6275885e-01,  5.2764446e-01]],\n",
       "\n",
       "       [[-1.7367585e-01, -1.2819721e+00,  7.0589316e-01,  1.0535363e+00],\n",
       "        [-1.2641865e+00,  8.0065459e-01, -1.2275412e+00, -1.3129770e+00]],\n",
       "\n",
       "       [[-1.1430187e+00, -1.5133747e+00, -2.6082402e-01, -2.6119328e-01],\n",
       "        [-1.0218509e+00,  8.0065459e-01, -1.2275412e+00, -1.0500311e+00]],\n",
       "\n",
       "       [[-1.3853549e+00,  3.3784863e-01, -1.2275412e+00, -1.3129770e+00],\n",
       "        [ 1.7650111e+00, -3.5636008e-01,  1.4451476e+00,  7.9059029e-01]],\n",
       "\n",
       "       [[-2.9484364e-01, -5.8776331e-01,  6.4902753e-01,  1.0535363e+00],\n",
       "        [-2.9484364e-01, -8.1916606e-01,  2.5096732e-01,  1.3322552e-01]],\n",
       "\n",
       "       [[ 1.6438427e+00, -1.2495735e-01,  1.1608191e+00,  5.2764446e-01],\n",
       "        [-9.0068316e-01,  8.0065459e-01, -1.2844069e+00, -1.3129770e+00]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 540,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.reshape(-1, 2, 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch19",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
