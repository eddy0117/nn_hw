{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class BasicNNModule:\n",
    "    def __init__(self):\n",
    "        self.lr = 0.001\n",
    "\n",
    "class SingleLinear(BasicNNModule):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.weights = np.ones((out_features, in_features)) * 0.5\n",
    "        self.bias = np.zeros(out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return np.matmul(x, self.weights.T) + self.bias\n",
    "    \n",
    "    def backward(self, delta_next_layer, w_next_layer, b_next_layer, a_cur_layer, y_last_layer):\n",
    "        # print(d2y.shape, y2i.shape, y_last_layer.shape)\n",
    "        # delta = d2y * y2i\n",
    "        delta = (np.matmul(delta_next_layer.T, w_next_layer) * a_cur_layer).T\n",
    "        # print(delta)\n",
    "        # L2 regularization\n",
    "        self.weights += self.lr * (np.matmul(delta, y_last_layer) - 0.01 * self.weights)\n",
    "        # self.weights += self.lr * np.matmul(delta, y_last_layer)\n",
    "        self.bias += self.lr * delta.T[0]\n",
    "        return delta, self.weights, self.bias\n",
    "    \n",
    "    def softmax_backward(self, y, label, y_last_layer):\n",
    "        delta = (y - label).T\n",
    "        # print('before update:', self.weights, self.bias)\n",
    "        # print(delta.shape, y_last_layer.shape, self.bias.shape)\n",
    "\n",
    "        # L2 regularization\n",
    "        self.weights += self.lr * (np.matmul(delta, y_last_layer) - 0.01 * self.weights)\n",
    "        # self.weights += self.lr * np.matmul(delta, y_last_layer)\n",
    "\n",
    "        self.bias += self.lr * delta.T[0]\n",
    "        # print('after update:', self.weights, self.bias)\n",
    "        return delta, self.weights, self.bias\n",
    "\n",
    "class relu:\n",
    "    def forward(self, x):\n",
    "        return np.maximum(x, 0)\n",
    "    \n",
    "    def backward(self, x):\n",
    "        return np.where(x > 0, 1, 0)\n",
    "    \n",
    "class Softmax:\n",
    "    def forward(self, x):\n",
    "        # 減去最大值以防止 overflow\n",
    "        x = x - np.max(x, axis=1, keepdims=True)\n",
    "        exp_x = np.exp(x)\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "with open('Iris.csv') as f:\n",
    "    data = f.readlines()[1:]\n",
    "\n",
    "data = [line.strip().split(',')[1:] for line in data]\n",
    "data = np.array(data)\n",
    "labels = []\n",
    "\n",
    "classes = np.unique(data[:, 4])\n",
    "\n",
    "for d in data:\n",
    "    for cls in classes:\n",
    "        if d[4] == cls:\n",
    "            one_hot = np.zeros(len(classes))\n",
    "            one_hot[classes.tolist().index(cls)] = 1\n",
    "            labels.append(one_hot)\n",
    "\n",
    "inputs = data[:, :4].astype(np.float32)\n",
    "\n",
    "# inputs = (inputs - np.mean(inputs, axis=0)) / np.std(inputs, axis=0)\n",
    "\n",
    "labels = np.array(labels)\n",
    "\n",
    "# 打亂數據\n",
    "idx = np.random.permutation(len(inputs))\n",
    "inputs = inputs[idx]\n",
    "labels = labels[idx]\n",
    "\n",
    "\n",
    "linear_1 = SingleLinear(4, 5)\n",
    "relu_1 = relu()\n",
    "linear_2 = SingleLinear(5, 5)\n",
    "relu_2 = relu()\n",
    "linear_3 = SingleLinear(5, 3)\n",
    "softmax_3 = Softmax()\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    for input, label in zip(inputs, labels):\n",
    "        input = input.reshape(1, -1)\n",
    "        # print(input, label)\n",
    "        i_1 = linear_1.forward(input)\n",
    "        y_1 = relu_1.forward(i_1)\n",
    "        i_2 = linear_2.forward(y_1)\n",
    "        y_2 = relu_2.forward(i_2)\n",
    "        i_3 = linear_3.forward(y_2)\n",
    "        print(\"i_3\", i_3)\n",
    "        y_3 = softmax_3.forward(i_3)\n",
    "        print(\"y_3\", y_3)\n",
    "        print(\"label\", label)\n",
    "        delta_3, w_3, b_3 = linear_3.softmax_backward(y_3, label, y_2)\n",
    "        delta_2, w_2, b_2 = linear_2.backward(delta_3, w_3, b_3, relu_2.backward(i_2), y_1)\n",
    "        delta_1, w_1, b_1 = linear_1.backward(delta_2, w_2, b_2, relu_1.backward(i_1), input)\n",
    "        # print('delta_3:\\n', delta_3)\n",
    "        # print('w_3:\\n', w_3)\n",
    "        # print('b_3:\\n', b_3)\n",
    "        \n",
    "        # print('delta_2:\\n', delta_2)\n",
    "        # print('w_2:\\n', w_2)\n",
    "        # print('b_2:\\n', b_2)\n",
    "        # break\n",
    "        # delta_1 = linear_1.backward(- ())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(delta_3.T, w_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01714783, 0.93623955, 0.04661262]])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Softmax:\n",
    "    def forward(self, x):\n",
    "        # 減去最大值以防止 overflow\n",
    "        x = x - np.max(x, axis=1, keepdims=True)\n",
    "        exp_x = np.exp(x)\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "Softmax().forward(np.array([[1, 5, 2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.95799297  0.50648367 -1.29696496  0.99630143]\n",
      " [-0.48203369  0.28668749  2.12959588  0.65556684]\n",
      " [-0.08816886  0.089359   -0.41808245  0.38364675]]\n",
      "[ 0.09246073  0.36246263  0.09009849 -0.19246959]\n",
      "[-0.21360764  0.12504099 -0.08727179]\n"
     ]
    }
   ],
   "source": [
    "a = np.random.randn(3, 4)\n",
    "b = np.random.randn(4)\n",
    "print(a)\n",
    "print(b)\n",
    "print(a.dot(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 150), (3, 150))"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "with open('Iris.csv') as f:\n",
    "    data = f.readlines()[1:]\n",
    "\n",
    "data = [line.strip().split(',')[1:] for line in data]\n",
    "data = np.array(data)\n",
    "labels = []\n",
    "\n",
    "classes = np.unique(data[:, 4])\n",
    "\n",
    "for d in data:\n",
    "    for cls in classes:\n",
    "        if d[4] == cls:\n",
    "            one_hot = np.zeros(len(classes))\n",
    "            one_hot[classes.tolist().index(cls)] = 1\n",
    "            labels.append(one_hot)\n",
    "\n",
    "inputs = data[:, :4].astype(np.float32)\n",
    "\n",
    "# inputs = (inputs - np.mean(inputs, axis=0)) / np.std(inputs, axis=0)\n",
    "\n",
    "labels = np.array(labels)\n",
    "\n",
    "# 打亂數據\n",
    "idx = np.random.permutation(len(inputs))\n",
    "inputs = inputs[idx].T\n",
    "labels = labels[idx].T\n",
    "\n",
    "\n",
    "inputs.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m: 4 n: 150\n"
     ]
    }
   ],
   "source": [
    "m, n = inputs.T.shape\n",
    "\n",
    "print('m:', m, 'n:', n)\n",
    "\n",
    "def init_params():\n",
    "    W1 = np.random.rand(10, 4) - 0.5\n",
    "    b1 = np.random.rand(10, 1) - 0.5\n",
    "    W2 = np.random.rand(3, 10) - 0.5\n",
    "    b2 = np.random.rand(3, 1) - 0.5\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(Z, 0)\n",
    "\n",
    "def relu_deriv(Z):\n",
    "    return Z > 0\n",
    "\n",
    "def softmax(Z):\n",
    "    A = np.exp(Z) / sum(np.exp(Z))\n",
    "    return A\n",
    "    \n",
    "def forward_prop(W1, b1, W2, b2, X):\n",
    "    # print('X shape:', X.shape, 'W1 shape:', W1.shape)\n",
    "    Z1 = np.matmul(W1, X) + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.matmul(W2, A1) + b2\n",
    "    A2 = softmax(Z2)\n",
    "    return Z1, A1, Z2, A2\n",
    "\n",
    "\n",
    "\n",
    "def one_hot(Y):\n",
    "    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "    one_hot_Y = one_hot_Y.T\n",
    "    return one_hot_Y\n",
    "\n",
    "def backward_prop(Z1, A1, A2, W2, X, Y):\n",
    "    one_hot_Y = one_hot(Y)\n",
    "    print('A2 shape:', A2.shape, 'Y shape:', Y.shape, 'one_hot_Y shape:', one_hot_Y.shape)\n",
    "    dZ2 = A2 - one_hot_Y\n",
    "    # dW2 = 1 / m * dZ2.dot(A1.T)\n",
    "    # print('dZ2 shape:', dZ2.shape, 'A1 shape:', A1.shape)\n",
    "    dW2 = (1 / m) * np.matmul(dZ2, A1.T)\n",
    "    # print(dW2.shape)\n",
    "    db2 = 1 / m * np.sum(dZ2)\n",
    "    dZ1 = W2.T.dot(dZ2) * relu_deriv(Z1)\n",
    "    # dW1 = 1 / m * dZ1.dot(X.T)\n",
    "    dW1 = (1 / m) * np.matmul(dZ1, X.T)\n",
    "    db1 = 1 / m * np.sum(dZ1)\n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n",
    "    W1 = W1 - alpha * dW1\n",
    "    b1 = b1 - alpha * db1    \n",
    "    W2 = W2 - alpha * dW2  \n",
    "    b2 = b2 - alpha * db2    \n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "def get_predictions(A2):\n",
    "    return np.argmax(A2, 0)\n",
    "\n",
    "def get_accuracy(predictions, Y):\n",
    "    # print(predictions, Y)\n",
    "    return np.sum(predictions == Y) / Y.size\n",
    "\n",
    "def gradient_descent(X, Y, alpha, iterations):\n",
    "    W1, b1, W2, b2 = init_params()\n",
    "    for i in range(iterations):\n",
    "        # for X, Y in zip(X_all.T, Y_all.T):\n",
    "        #     X = X.reshape(-1, 1)\n",
    "        #     Y = Y.reshape(-1, 1)\n",
    "            # print('Y shape:', Y.shape, 'Y_all shape:', Y_all.shape)\n",
    "            # print('X_all shape:', X_all.shape)\n",
    "            # print('X shape:', X.shape)\n",
    "        Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X)\n",
    "        dW1, db1, dW2, db2 = backward_prop(Z1, A1, A2, W2, X, Y)\n",
    "        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
    "        if i % 10 == 0:\n",
    "            print(\"Iteration: \", i)\n",
    "\n",
    "            predictions = get_predictions(A2)\n",
    "            print(get_accuracy(predictions, Y))\n",
    "        break\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 150 is different from 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m Y_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(labels, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Y_train = labels\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m W1, b1, W2, b2 \u001b[38;5;241m=\u001b[39m \u001b[43mgradient_descent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 76\u001b[0m, in \u001b[0;36mgradient_descent\u001b[0;34m(X, Y, alpha, iterations)\u001b[0m\n\u001b[1;32m     68\u001b[0m W1, b1, W2, b2 \u001b[38;5;241m=\u001b[39m init_params()\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(iterations):\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m# for X, Y in zip(X_all.T, Y_all.T):\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m#     X = X.reshape(-1, 1)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;66;03m# print('X_all shape:', X_all.shape)\u001b[39;00m\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;66;03m# print('X shape:', X.shape)\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m     Z1, A1, Z2, A2 \u001b[38;5;241m=\u001b[39m \u001b[43mforward_prop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     dW1, db1, dW2, db2 \u001b[38;5;241m=\u001b[39m backward_prop(Z1, A1, A2, W2, X, Y)\n\u001b[1;32m     78\u001b[0m     W1, b1, W2, b2 \u001b[38;5;241m=\u001b[39m update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
      "Cell \u001b[0;32mIn[5], line 24\u001b[0m, in \u001b[0;36mforward_prop\u001b[0;34m(W1, b1, W2, b2, X)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_prop\u001b[39m(W1, b1, W2, b2, X):\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# print('X shape:', X.shape, 'W1 shape:', W1.shape)\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     Z1 \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m b1\n\u001b[1;32m     25\u001b[0m     A1 \u001b[38;5;241m=\u001b[39m relu(Z1)\n\u001b[1;32m     26\u001b[0m     Z2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmatmul(W2, A1) \u001b[38;5;241m+\u001b[39m b2\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 150 is different from 4)"
     ]
    }
   ],
   "source": [
    "X_train = inputs\n",
    "Y_train = np.argmax(labels, axis=0)\n",
    "# Y_train = labels\n",
    "W1, b1, W2, b2 = gradient_descent(X_train, Y_train, 0.10, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m: 150 n: 4\n"
     ]
    }
   ],
   "source": [
    "m, n = inputs.T.shape\n",
    "\n",
    "print('m:', m, 'n:', n)\n",
    "\n",
    "def init_params():\n",
    "    W1 = np.random.rand(10, 4) - 0.5\n",
    "    b1 = np.random.rand(10, 1) - 0.5\n",
    "    W2 = np.random.rand(3, 10) - 0.5\n",
    "    b2 = np.random.rand(3, 1) - 0.5\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(Z, 0)\n",
    "\n",
    "def relu_deriv(Z):\n",
    "    return Z > 0\n",
    "\n",
    "def softmax(Z):\n",
    "    A = np.exp(Z) / sum(np.exp(Z))\n",
    "    return A\n",
    "    \n",
    "def forward_prop(W1, b1, W2, b2, X):\n",
    "    # print('X shape:', X.shape, 'W1 shape:', W1.shape)\n",
    "    Z1 = np.matmul(W1, X) + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.matmul(W2, A1) + b2\n",
    "    A2 = softmax(Z2)\n",
    "    return Z1, A1, Z2, A2\n",
    "\n",
    "\n",
    "\n",
    "def one_hot(Y):\n",
    "    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "    one_hot_Y = one_hot_Y.T\n",
    "    return one_hot_Y\n",
    "\n",
    "def backward_prop(Z1, A1, A2, W2, X, Y):\n",
    "    # one_hot_Y = Y.reshape(-1, 1)\n",
    "    one_hot_Y = one_hot(Y)\n",
    "    # print('A2 shape:', A2.shape, 'Y shape:', Y.shape, 'one_hot_Y shape:', one_hot_Y.shape)\n",
    "    dZ2 = A2 - one_hot_Y\n",
    "    # dW2 = 1 / m * dZ2.dot(A1.T)\n",
    "    # print('dZ2 shape:', dZ2.shape, 'A1 shape:', A1.shape)\n",
    "    dW2 = (1 / m) * np.matmul(dZ2, A1.T)\n",
    "    # print(dW2.shape)\n",
    "    db2 = (1 / m) * np.sum(dZ2)\n",
    "    dZ1 = W2.T.dot(dZ2) * relu_deriv(Z1)\n",
    "    # dW1 = 1 / m * dZ1.dot(X.T)\n",
    "    dW1 = (1 / m) * np.matmul(dZ1, X.T)\n",
    "    db1 = 1 / m * np.sum(dZ1)\n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n",
    "    W1 = W1 - alpha * dW1\n",
    "    b1 = b1 - alpha * db1    \n",
    "    W2 = W2 - alpha * dW2  \n",
    "    b2 = b2 - alpha * db2    \n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "def get_predictions(A2):\n",
    "    return np.argmax(A2, 0)\n",
    "\n",
    "def get_accuracy(predictions, Y):\n",
    "    # print(predictions, Y)\n",
    "    return np.sum(predictions == Y) / Y.size\n",
    "\n",
    "def gradient_descent(X, Y, alpha, iterations):\n",
    "    bs = 10\n",
    "    W1, b1, W2, b2 = init_params()\n",
    "    for i in range(iterations):\n",
    "        # for X, Y_s in zip(X_all.T, Y_all.T):\n",
    "\n",
    "        #     X = X.reshape(-1, 1)\n",
    "        #     Y = Y.reshape(-1, 1)\n",
    "        # print('Y shape:', Y.shape, 'Y_all shape:', Y_all.shape)\n",
    "        # print('X_all shape:', X_all.shape)\n",
    "        # print('X shape:', X.shape)\n",
    "        Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X)\n",
    "        dW1, db1, dW2, db2 = backward_prop(Z1, A1, A2, W2, X, Y)\n",
    "        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
    "        if i % 50 == 0:\n",
    "            print(\"Iteration: \", i)\n",
    "\n",
    "            predictions = get_predictions(A2)\n",
    "            print(get_accuracy(predictions, Y))\n",
    "        # break\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "0.3333333333333333\n",
      "Iteration:  50\n",
      "0.9133333333333333\n",
      "Iteration:  100\n",
      "0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "X_train = inputs\n",
    "Y_train = np.argmax(labels, axis=0)\n",
    "# Y_train = labels\n",
    "W1, b1, W2, b2 = gradient_descent(X_train, Y_train, 0.05, 101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ======================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params():\n",
    "    W1 = np.ones((4, 10))\n",
    "    b1 = np.zeros((1, 10))\n",
    "    W2 = np.ones((10, 3))\n",
    "    b2 = np.zeros((1, 3))\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "def d_relu(x):\n",
    "    return x > 0\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "def forward_prop(W1, b1, W2, b2, X):\n",
    "\n",
    "    # X shape: (bs, n_features)\n",
    "    I1 = np.matmul(X, W1) + b1  \n",
    "    Y1 = relu(I1)\n",
    "\n",
    "    I2 = np.matmul(Y1, W2) + b2\n",
    "    Y2 = softmax(I2)\n",
    "    return I1, Y1, I2, Y2\n",
    "\n",
    "def backward_prop(I1, Y1, Y2, W2, X, Y):\n",
    "\n",
    "    # 設定標準化因子 (batch size的倒數) 將梯度標準化，避免 batch size 過大造成梯度累積過大\n",
    "    m = X.shape[0]\n",
    "    norm_factor = 1 / m\n",
    "    \n",
    "    delta_2 = Y2 - Y  # (bs, 3)\n",
    "\n",
    "    # Y1.T shape -> (n_layer1, bs)\n",
    "    # delta_2 shape -> (bs, n_layer2)\n",
    "\n",
    "    dW2 = norm_factor * np.matmul(Y1.T, delta_2)  \n",
    "    db2 = norm_factor * np.sum(delta_2, axis=0, keepdims=True)\n",
    "\n",
    "    # dW2 shape -> (n_layer1, n_layer2)\n",
    "    # db2 shape -> (1, n_layer2)\n",
    "\n",
    "    # ========================================\n",
    "\n",
    "    # delta_2 shape -> (bs, n_layer2)\n",
    "    # W2.T shape -> (n_layer2, n_layer1)\n",
    "    # d_relu(I1) shape -> (bs, n_layer1)\n",
    "\n",
    "    delta_1 = np.matmul(delta_2, W2.T) * d_relu(I1)  \n",
    " \n",
    "    # X.T shape -> (n_features, bs)\n",
    "    # delta_1 shape -> (bs, n_layer1)\n",
    "    \n",
    "    dW1 = norm_factor * np.matmul(X.T, delta_1)  \n",
    "    db1 = norm_factor * np.sum(delta_1, axis=0, keepdims=True)\n",
    "\n",
    "    # dW1 shape -> (n_features, n_layer1)\n",
    "    # db1 shape -> (1, n_layer1)\n",
    "    \n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, lr):\n",
    "    W1 = W1 - lr * dW1\n",
    "    b1 = b1 - lr * db1\n",
    "    W2 = W2 - lr * dW2\n",
    "    b2 = b2 - lr * db2\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "def get_predictions(A2):\n",
    "    return np.argmax(A2, axis=1)\n",
    "\n",
    "def get_accuracy(predictions, Y):\n",
    "    Y = np.argmax(Y, axis=1)\n",
    "    return np.sum(predictions == Y) / Y.size\n",
    "\n",
    "def gradient_descent(X_all, Y_all, X_val, Y_val, alpha, iterations, bs):\n",
    "    # X_all -> (n_samples, n_features)\n",
    "    # Y_all -> (n_samples, n_classes) one-hot \n",
    "    \n",
    "    W1, b1, W2, b2 = init_params()\n",
    "    \n",
    "    # 將全部的資料打包成 batch，每個 batch 的大小為 bs\n",
    "    X_batch = X_all.reshape(-1, bs, X_all.shape[1])\n",
    "    Y_batch = Y_all.reshape(-1, bs, Y_all.shape[1])\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        loss = 0\n",
    "        for X, Y in zip(X_batch, Y_batch):\n",
    "          \n",
    "            # print('Y shape:', Y.shape, 'Y_all shape:', Y_all.shape)\n",
    "            I1, Y1, I2, Y2 = forward_prop(W1, b1, W2, b2, X)\n",
    "            dW1, db1, dW2, db2 = backward_prop(I1, Y1, Y2, W2, X, Y)\n",
    "            W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
    "            loss += -np.sum(Y * np.log(Y2))\n",
    "\n",
    "        if i % 1 == 0:\n",
    "            print(\"Iteration: \", i)\n",
    "            print(\"Loss: \", loss)\n",
    "            _, _, _, pred = forward_prop(W1, b1, W2, b2, X_val)\n",
    "            predictions = get_predictions(pred)\n",
    "            print('Acc:', get_accuracy(predictions, Y_val))\n",
    "            # print(predictions, Y)\n",
    "        break\n",
    "\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 4)\n",
      "d_relu(I1): (120, 10)\n",
      "Iteration:  0\n",
      "Loss:  131.83347464017316\n",
      "Acc: 0.36666666666666664\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 固定隨機種子，確保同參數下每次結果都相同\n",
    "np.random.seed(1)\n",
    "\n",
    "with open('Iris.csv') as f:\n",
    "    data = f.readlines()[1:]\n",
    "\n",
    "data = [line.strip().split(',')[1:] for line in data]\n",
    "data = np.array(data)\n",
    "labels = []\n",
    "\n",
    "classes = np.unique(data[:, 4])\n",
    "\n",
    "for d in data:\n",
    "    for cls in classes:\n",
    "        if d[4] == cls:\n",
    "            one_hot = np.zeros(len(classes))\n",
    "            one_hot[classes.tolist().index(cls)] = 1\n",
    "            labels.append(one_hot)\n",
    "\n",
    "inputs = data[:, :4].astype(np.float32)\n",
    "\n",
    "# inputs = (inputs - np.mean(inputs, axis=0)) / np.std(inputs, axis=0)\n",
    "\n",
    "labels = np.array(labels)\n",
    "\n",
    "# 打亂數據\n",
    "idx = np.random.permutation(len(inputs))\n",
    "\n",
    "X_data = inputs[idx]\n",
    "# Y_train = np.argmax(labels, axis=0)\n",
    "\n",
    "Y_data = labels[idx]\n",
    "\n",
    "train_size = int(len(X_data) * 0.8)\n",
    "val_size = len(X_data) - train_size\n",
    "\n",
    "X_train, Y_train = X_data[:train_size], Y_data[:train_size]\n",
    "X_val, Y_val = X_data[val_size:], Y_data[val_size:]\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "batch_size = 120\n",
    "lr = 0.05\n",
    "\n",
    "W1, b1, W2, b2 = gradient_descent(X_train, Y_train, X_val, Y_val, lr, 30, batch_size)\n",
    "# W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.42146685 -1.49794256 -0.89849127 -0.66365005]\n",
      " [-0.51160228  0.93826069 -0.27637929 -0.5548736 ]\n",
      " [-0.35771471 -1.06233953 -0.41493013 -0.36709497]\n",
      " [ 3.28614577  4.57316731  1.2664861   0.41866526]]\n",
      "[[-2.42146685 -1.49794256 -0.89849127 -0.66365005]\n",
      " [-0.51160228  0.93826069 -0.27637929 -0.5548736 ]\n",
      " [-0.35771471 -1.06233953 -0.41493013 -0.36709497]\n",
      " [ 3.28614577  4.57316731  1.2664861   0.41866526]]\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(4, 3)\n",
    "y = np.random.randn(3, 4)\n",
    "\n",
    "print(np.matmul(y.T, x.T))\n",
    "print(np.matmul(x, y).T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## =================== 改良 =====================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params():\n",
    "    params = {'w': [], 'b': [], 'act_func': []}\n",
    "    W1 = np.random.randn(4, 10) * np.sqrt(2/4)\n",
    "    b1 = np.zeros((1, 10))\n",
    "    \n",
    "    W2 = np.random.randn(10, 5) * np.sqrt(2/10)\n",
    "    b2 = np.zeros((1, 5))\n",
    "    \n",
    "    W3 = np.random.randn(5, 3) * np.sqrt(2/5)\n",
    "    b3 = np.zeros((1, 3))\n",
    "    # W3 = np.ones((5, 3))\n",
    "    # b3 = np.zeros((1, 3))\n",
    "\n",
    "    params['w'].append(W1)\n",
    "    params['b'].append(b1)\n",
    "    params['act_func'].append(relu)\n",
    "    params['w'].append(W2)\n",
    "    params['b'].append(b2)\n",
    "    params['act_func'].append(relu)\n",
    "    params['w'].append(W3)\n",
    "    params['b'].append(b3)\n",
    "    params['act_func'].append(softmax)\n",
    "    return params\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "def d_relu(x):\n",
    "    return x > 0\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "def forward_prop(params, X):\n",
    "    forward_val = {'I': [], 'Y': []}\n",
    "    # X shape: (bs, n_features)\n",
    "\n",
    "    for idx, (w, b, act_func) in enumerate(zip(params['w'], params['b'], params['act_func'])):\n",
    "        # print('forward idx:', idx, 'X shape:', X.shape, 'w', w[0])\n",
    "        if idx == 0:\n",
    "            I = np.matmul(X, w) + b\n",
    "        else:\n",
    "            I = np.matmul(Y, w) + b\n",
    "        Y = act_func.f(I)\n",
    "        # print('idx:', idx, 'I shape:', I.shape, 'Y shape:', Y.shape)\n",
    "        forward_val['I'].append(I)\n",
    "        forward_val['Y'].append(Y)\n",
    "\n",
    "    # I1 = np.matmul(X, W1) + b1  \n",
    "    # Y1 = relu(I1)\n",
    "\n",
    "    # I2 = np.matmul(Y1, W2) + b2\n",
    "    # Y2 = softmax(I2)\n",
    "    return forward_val\n",
    "\n",
    "def backward_prop(params: dict, forward_val: dict, input_feat, label) -> dict:\n",
    "\n",
    "    params_delta = {'W': [], 'b': []}\n",
    "\n",
    "    # 設定標準化因子 (batch size的倒數) 將梯度標準化，避免 batch size 過大造成梯度累積過大\n",
    "    m = input_feat.shape[0]\n",
    "    norm_factor = 1 / m\n",
    "    n_layer = len(params['w'])\n",
    "\n",
    "    for idx, (Y, I, W, act_func) in enumerate(zip(forward_val['Y'][::-1], forward_val['I'][::-1], params['w'][::-1], params['act_func'][::-1])):\n",
    "        # 由 output layer 開始計算 delta params\n",
    "\n",
    "        # 當目前為 output layer 時\n",
    "        # delta 為 dL/dY * dY/dI -> Cross Entropy Loss 與 Softmax 的導函數相乘\n",
    "        # 可化簡為 Y - label\n",
    "        \n",
    "        if idx == 0:\n",
    "            delta = Y - label\n",
    "        else:\n",
    "            # print('idx', idx, 'delta shape:', delta.shape, 'W shape:', W.shape)\n",
    "            delta = np.matmul(delta, next_layer_w.T) * act_func.b(I)\n",
    "       \n",
    "\n",
    "        # 當目前為 input layer 時，dW 的左矩陣為 input_feat.T\n",
    "        if idx == n_layer - 1:\n",
    "            dW = norm_factor * np.matmul(input_feat.T, delta)\n",
    "        else:\n",
    "            dW = norm_factor * np.matmul(forward_val['Y'][::-1][idx+1].T, delta)\n",
    "            \n",
    "        db = norm_factor * np.sum(delta, axis=0, keepdims=True)\n",
    "\n",
    "        \n",
    "        params_delta['W'].append(dW)\n",
    "        params_delta['b'].append(db)\n",
    "        next_layer_w = W\n",
    "    \n",
    "    # delta_2 = Y2 - Y  # (bs, 3)\n",
    "\n",
    "    # # Y1.T shape -> (n_layer1, bs)\n",
    "    # # delta_2 shape -> (bs, n_layer2)\n",
    "\n",
    "    # dW2 = norm_factor * np.matmul(Y1.T, delta_2)  \n",
    "    # db2 = norm_factor * np.sum(delta_2, axis=0, keepdims=True)\n",
    "\n",
    "    # # dW2 shape -> (n_layer1, n_layer2)\n",
    "    # # db2 shape -> (1, n_layer2)\n",
    "\n",
    "    # # ========================================\n",
    "\n",
    "    # # delta_2 shape -> (bs, n_layer2)\n",
    "    # # W2.T shape -> (n_layer2, n_layer1)\n",
    "    # # d_relu(I1) shape -> (bs, n_layer1)\n",
    "\n",
    "    # delta_1 = np.matmul(delta_2, W2.T) * d_relu(I1)  \n",
    " \n",
    "    # # X.T shape -> (n_features, bs)\n",
    "    # # delta_1 shape -> (bs, n_layer1)\n",
    "    \n",
    "    # dW1 = norm_factor * np.matmul(X.T, delta_1)  \n",
    "    # db1 = norm_factor * np.sum(delta_1, axis=0, keepdims=True)\n",
    "\n",
    "    # # dW1 shape -> (n_features, n_layer1)\n",
    "    # # db1 shape -> (1, n_layer1)\n",
    "    \n",
    "    # params_delta['W'].append(dW1)\n",
    "    # params_delta['b'].append(db1)\n",
    "    # params_delta['W'].append(dW2)\n",
    "    # params_delta['b'].append(db2)\n",
    "\n",
    "    return params_delta\n",
    "\n",
    "def update_params(params: dict, params_delta: dict, lr) -> dict:\n",
    "    # W1 = W1 - lr * dW1\n",
    "    # b1 = b1 - lr * db1\n",
    "    # W2 = W2 - lr * dW2\n",
    "    # b2 = b2 - lr * db2\n",
    "    \n",
    "    for idx, (W, b, dW, db) in enumerate(zip(params['w'], params['b'], params_delta['W'][::-1], params_delta['b'][::-1])):\n",
    "        # print('idx:', idx, 'W shape:', W.shape, 'b shape:', b.shape, 'dW shape:', dW.shape, 'db shape:', db.shape)\n",
    "        W = W - lr * dW\n",
    "        b = b - lr * db\n",
    "        params['w'][idx] = W\n",
    "        params['b'][idx] = b\n",
    "\n",
    "    return params\n",
    "\n",
    "def get_predictions(A2):\n",
    "    return np.argmax(A2, axis=1)\n",
    "\n",
    "def get_accuracy(predictions, Y):\n",
    "    Y = np.argmax(Y, axis=1)\n",
    "    return np.sum(predictions == Y) / Y.size\n",
    "\n",
    "def gradient_descent(params, X_all, Y_all, X_val, Y_val, alpha, iterations, bs):\n",
    "    # X_all -> (n_samples, n_features)\n",
    "    # Y_all -> (n_samples, n_classes) one-hot \n",
    "    \n",
    "    # params = init_params()\n",
    "    \n",
    "    # 將全部的資料打包成 batch，每個 batch 的大小為 bs\n",
    "    X_batch = X_all.reshape(-1, bs, X_all.shape[1])\n",
    "    Y_batch = Y_all.reshape(-1, bs, Y_all.shape[1])\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        loss = 0\n",
    "        for X, Y in zip(X_batch, Y_batch):\n",
    "          \n",
    "            # print('Y shape:', Y.shape, 'Y_all shape:', Y_all.shape)\n",
    "            forward_val = forward_prop(params, X)\n",
    "            params_delta = backward_prop(params, forward_val, X, Y)\n",
    "            params = update_params(params, params_delta, alpha)\n",
    "            loss += -np.sum(Y * np.log(forward_val['Y'][-1]))\n",
    "\n",
    "        if i % 1 == 0:\n",
    "            print(\"Iteration: \", i)\n",
    "            print('Loss:', round(loss, 2))\n",
    "            pred = forward_prop(params, X_val)\n",
    "            predictions = get_predictions(pred['Y'][-1])\n",
    "            print('Val Acc:', round(get_accuracy(predictions, Y_val), 2))\n",
    "            # print(predictions, Y)\n",
    "        # break\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "class ReLU:\n",
    "    def f(x):\n",
    "        return np.maximum(x, 0)\n",
    "    def b(x):\n",
    "        return x > 0\n",
    "    \n",
    "class Softmax:\n",
    "    def f(x):\n",
    "        exp_x = np.exp(x)\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 4)\n",
      "Iteration:  0\n",
      "Loss: 137.85\n",
      "Val Acc: 0.69\n",
      "Iteration:  1\n",
      "Loss: 84.05\n",
      "Val Acc: 0.69\n",
      "Iteration:  2\n",
      "Loss: 67.35\n",
      "Val Acc: 0.69\n",
      "Iteration:  3\n",
      "Loss: 60.1\n",
      "Val Acc: 0.69\n",
      "Iteration:  4\n",
      "Loss: 55.08\n",
      "Val Acc: 0.69\n",
      "Iteration:  5\n",
      "Loss: 51.87\n",
      "Val Acc: 0.7\n",
      "Iteration:  6\n",
      "Loss: 51.15\n",
      "Val Acc: 0.72\n",
      "Iteration:  7\n",
      "Loss: 50.16\n",
      "Val Acc: 0.72\n",
      "Iteration:  8\n",
      "Loss: 47.68\n",
      "Val Acc: 0.72\n",
      "Iteration:  9\n",
      "Loss: 46.04\n",
      "Val Acc: 0.72\n",
      "Iteration:  10\n",
      "Loss: 44.18\n",
      "Val Acc: 0.74\n",
      "Iteration:  11\n",
      "Loss: 44.2\n",
      "Val Acc: 0.75\n",
      "Iteration:  12\n",
      "Loss: 41.39\n",
      "Val Acc: 0.84\n",
      "Iteration:  13\n",
      "Loss: 40.13\n",
      "Val Acc: 0.85\n",
      "Iteration:  14\n",
      "Loss: 39.35\n",
      "Val Acc: 0.85\n",
      "Iteration:  15\n",
      "Loss: 39.47\n",
      "Val Acc: 0.84\n",
      "Iteration:  16\n",
      "Loss: 39.21\n",
      "Val Acc: 0.84\n",
      "Iteration:  17\n",
      "Loss: 41.41\n",
      "Val Acc: 0.79\n",
      "Iteration:  18\n",
      "Loss: 35.22\n",
      "Val Acc: 0.75\n",
      "Iteration:  19\n",
      "Loss: 37.96\n",
      "Val Acc: 0.83\n",
      "Iteration:  20\n",
      "Loss: 32.62\n",
      "Val Acc: 0.91\n",
      "Iteration:  21\n",
      "Loss: 41.35\n",
      "Val Acc: 0.74\n",
      "Iteration:  22\n",
      "Loss: 37.03\n",
      "Val Acc: 0.88\n",
      "Iteration:  23\n",
      "Loss: 28.11\n",
      "Val Acc: 0.95\n",
      "Iteration:  24\n",
      "Loss: 40.78\n",
      "Val Acc: 0.95\n",
      "Iteration:  25\n",
      "Loss: 25.04\n",
      "Val Acc: 0.92\n",
      "Iteration:  26\n",
      "Loss: 48.13\n",
      "Val Acc: 0.74\n",
      "Iteration:  27\n",
      "Loss: 36.07\n",
      "Val Acc: 0.88\n",
      "Iteration:  28\n",
      "Loss: 35.76\n",
      "Val Acc: 0.88\n",
      "Iteration:  29\n",
      "Loss: 25.68\n",
      "Val Acc: 0.73\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 固定隨機種子，確保同參數下每次結果都相同\n",
    "np.random.seed(1)\n",
    "\n",
    "with open('Iris.csv') as f:\n",
    "    data = f.readlines()[1:]\n",
    "\n",
    "data = [line.strip().split(',')[1:] for line in data]\n",
    "data = np.array(data)\n",
    "labels = []\n",
    "\n",
    "classes = np.unique(data[:, 4])\n",
    "\n",
    "for d in data:\n",
    "    for cls in classes:\n",
    "        if d[4] == cls:\n",
    "            one_hot = np.zeros(len(classes))\n",
    "            one_hot[classes.tolist().index(cls)] = 1\n",
    "            labels.append(one_hot)\n",
    "\n",
    "inputs = data[:, :4].astype(np.float32)\n",
    "\n",
    "# inputs = (inputs - np.mean(inputs, axis=0)) / np.std(inputs, axis=0)\n",
    "\n",
    "labels = np.array(labels)\n",
    "\n",
    "# 打亂數據\n",
    "idx = np.random.permutation(len(inputs))\n",
    "\n",
    "X_data = inputs[idx]\n",
    "# Y_train = np.argmax(labels, axis=0)\n",
    "\n",
    "Y_data = labels[idx]\n",
    "\n",
    "train_size = int(len(X_data) * 0.8)\n",
    "val_size = len(X_data) - train_size\n",
    "\n",
    "X_train, Y_train = X_data[:train_size], Y_data[:train_size]\n",
    "X_val, Y_val = X_data[val_size:], Y_data[val_size:]\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "batch_size = 3\n",
    "lr = 0.05\n",
    "\n",
    "\n",
    "init_params = {\n",
    "    'w': [\n",
    "        np.random.randn(4, 5) * np.sqrt(2/4),\n",
    "        np.random.randn(5, 5) * np.sqrt(2/10),\n",
    "        np.random.randn(5, 5) * np.sqrt(2/10),\n",
    "        np.random.randn(5, 3) * np.sqrt(2/5)\n",
    "        ],\n",
    "    'b': [\n",
    "        np.zeros((1, 5)),\n",
    "        np.zeros((1, 5)),\n",
    "        np.zeros((1, 5)),\n",
    "        np.zeros((1, 3))\n",
    "        ],\n",
    "    'act_func': [\n",
    "        ReLU,\n",
    "        ReLU, \n",
    "        ReLU,\n",
    "        Softmax\n",
    "                 ]\n",
    "    }\n",
    "\n",
    "params = gradient_descent(init_params, X_train, Y_train, X_val, Y_val, lr, 30, batch_size)\n",
    "# params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 4\n",
      "1 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3, 2, 1]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = {'a': [1, 2], 'b': [3, 4]}\n",
    "\n",
    "for x, y in reversed(list(zip(a['a'], a['b']))):\n",
    "    print(x, y)\n",
    "\n",
    "a = [1, 2, 3]\n",
    "# inverse a\n",
    "a[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch19",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
