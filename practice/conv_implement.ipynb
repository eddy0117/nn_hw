{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 筆記\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def im2col(input_feat: np.ndarray, N, kh, kw, out_h, out_w, stride):\n",
    "    im2col_feat = []\n",
    "    for n in range(N):\n",
    "        for ih in range(out_h):\n",
    "            for iw in range(out_w):\n",
    "                im2col_feat.append(input_feat[n, :, stride * ih:stride * ih + kh, stride * iw:stride * iw + kw])\n",
    "                # each element -> (C, kh, kw)\n",
    "    # input_feat -> (N*out_h*out_w, C, kh, kw)\n",
    "\n",
    "    return np.array(im2col_feat).reshape(N * out_h * out_w, -1)\n",
    "\n",
    "def convolution(input_feat: np.ndarray, filter: np.ndarray, kh, kw, stride=1, padding=0, bias=None):\n",
    "    '''\n",
    "    input_feat: (N, C, H, W)\n",
    "    filter: (out_C, in_C, kH, kw)\n",
    "    bias: (out_C, 1)\n",
    "    '''\n",
    "    N, C, H, W = input_feat.shape\n",
    "    out_h = int((H - kh + 2 * padding) // stride) + 1\n",
    "    out_w = int((W - kw + 2 * padding) // stride) + 1\n",
    "    out_c = filter.shape[0]\n",
    "    \n",
    "    if padding:\n",
    "        input_feat = np.pad(input_feat, ((0, 0), (0, 0), (padding, padding), (padding, padding)), 'constant', constant_values=0)\n",
    "\n",
    "    im2col_feat = im2col(input_feat, N, kh, kw, out_h, out_w, stride)\n",
    "    # im2col -> (N*out_h*out_w, C*kh*kw)\n",
    "\n",
    "    filter = filter.reshape(out_c, -1)\n",
    "    # filter -> (out_c, C*kh*kw)\n",
    "\n",
    "    # w @ x.T\n",
    "    # w -> (out_c, C*kh*kw)\n",
    "    # x.T -> (C*kh*kw, N*out_h*out_w)\n",
    "    if isinstance(bias, np.ndarray):\n",
    "        out_feat = filter @ im2col_feat.T + bias\n",
    "    else:\n",
    "        out_feat = filter @ im2col_feat.T\n",
    "    # out_feat -> (out_c, N*out_h*out_w)\n",
    "    \n",
    "    # 直接將 (out_c, N*out_h*out_w) reshape 成 (N, out_c, out_h, out_w) 會產生順序錯亂\n",
    "    # 所以先將 (out_c, N*out_h*out_w) 拆成 (out_c, N, out_h, out_w) 後再 permute\n",
    "    # out_feat -> (N, out_c, out_h, out_w)\n",
    "    return out_feat.reshape(out_c, N, out_h, out_w).transpose(1, 0, 2, 3)\n",
    "    # return out_feat.T.reshape(N, out_h, out_w, out_c).transpose(0, 3, 1, 2)\n",
    "   \n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input feat:\n",
      "[[[[ 2. 18. 11.]\n",
      "   [11. 19.  3.]\n",
      "   [ 7.  9.  9.]]\n",
      "\n",
      "  [[13. 13.  4.]\n",
      "   [ 3. 12.  4.]\n",
      "   [ 1. 10.  3.]]\n",
      "\n",
      "  [[12.  7.  2.]\n",
      "   [ 6. 10. 10.]\n",
      "   [ 5. 19. 12.]]]\n",
      "\n",
      "\n",
      " [[[ 5.  4. 11.]\n",
      "   [ 0. 15.  8.]\n",
      "   [ 8. 16.  4.]]\n",
      "\n",
      "  [[ 7. 16. 12.]\n",
      "   [ 6. 13.  5.]\n",
      "   [ 6. 14.  3.]]\n",
      "\n",
      "  [[15.  0. 15.]\n",
      "   [ 3. 18.  9.]\n",
      "   [ 9.  1.  4.]]]]\n",
      "======================\n",
      "filter\n",
      ":[[[[2. 0.]\n",
      "   [3. 4.]]\n",
      "\n",
      "  [[0. 1.]\n",
      "   [0. 0.]]\n",
      "\n",
      "  [[1. 1.]\n",
      "   [1. 4.]]]\n",
      "\n",
      "\n",
      " [[[1. 3.]\n",
      "   [1. 0.]]\n",
      "\n",
      "  [[3. 4.]\n",
      "   [1. 0.]]\n",
      "\n",
      "  [[1. 1.]\n",
      "   [0. 0.]]]\n",
      "\n",
      "\n",
      " [[[4. 3.]\n",
      "   [4. 1.]]\n",
      "\n",
      "  [[4. 4.]\n",
      "   [2. 1.]]\n",
      "\n",
      "  [[4. 4.]\n",
      "   [2. 1.]]]]\n",
      "======================\n",
      "my out:\n",
      "[[[[191. 168.]\n",
      "   [188. 192.]]\n",
      "\n",
      "  [[180. 146.]\n",
      "   [149. 119.]]\n",
      "\n",
      "  [[345. 346.]\n",
      "   [303. 347.]]]\n",
      "\n",
      "\n",
      " [[[176. 166.]\n",
      "   [135. 143.]]\n",
      "\n",
      "  [[123. 176.]\n",
      "   [150. 155.]]\n",
      "\n",
      "  [[248. 365.]\n",
      "   [298. 369.]]]]\n",
      "torch out:\n",
      "[[[[191. 168.]\n",
      "   [188. 192.]]\n",
      "\n",
      "  [[180. 146.]\n",
      "   [149. 119.]]\n",
      "\n",
      "  [[345. 346.]\n",
      "   [303. 347.]]]\n",
      "\n",
      "\n",
      " [[[176. 166.]\n",
      "   [135. 143.]]\n",
      "\n",
      "  [[123. 176.]\n",
      "   [150. 155.]]\n",
      "\n",
      "  [[248. 365.]\n",
      "   [298. 369.]]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# input feat (c, h, w) -> (3, 3, 3)\n",
    "# filter (out_c, in_c, kh, kw) -> (3, 3, 2, 2)\n",
    "# bias (out_c, 1)\n",
    "bs = 2\n",
    "feat_h = 3\n",
    "feat_w = 3\n",
    "kh = 2\n",
    "kw = 2\n",
    "in_c = 3\n",
    "out_c = 3\n",
    "padding = 0\n",
    "stride = 1\n",
    "\n",
    "input_feat = np.random.randint(0, 20, size=(bs, in_c, feat_h, feat_w)).astype(np.float32)\n",
    "filter = np.random.randint(0, 5, size=(out_c, in_c, kh, kw)).astype(np.float32)\n",
    "# bias = np.random.randint(0, 5, size=(2, 1))\n",
    "bias = np.zeros((out_c, 1), dtype=np.float32)\n",
    "\n",
    "out = convolution(input_feat, filter, kh=kh, kw=kw, stride=stride, padding=padding, bias=bias)\n",
    "\n",
    "# 與 pytorch 的實現方法對照結果\n",
    "out_t = F.conv2d(torch.tensor(input_feat), torch.tensor(filter), stride=stride, padding=padding, bias=torch.tensor(bias).squeeze(1))\n",
    "\n",
    "\n",
    "print(f'input feat:\\n{input_feat}')\n",
    "print('======================')\n",
    "print(f'filter\\n:{filter}')\n",
    "print('======================')\n",
    "print(f'my out:\\n{out}')\n",
    "print(f'torch out:\\n{out_t.numpy()}')\n",
    "np.allclose(out, out_t)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_mlp import MLP, ReLU, Softmax, CrossEntropyLoss\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyModel(MLP):\n",
    "    def __init__(self, \n",
    "                 in_channel: int,\n",
    "                 out_channel: int,\n",
    "                 kernel_size: tuple,\n",
    "                 stride: int,\n",
    "                 padding: int,\n",
    "                 bias=False\n",
    "                 ):\n",
    "        \n",
    "\n",
    "        kh, kw = kernel_size\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        params_set_list = [(in_channel * kh * kw, out_channel, ReLU),\n",
    "                           (8 * 26 * 26, 10, Softmax)]\n",
    "        self.params = self.weight_init(params_set_list)\n",
    "        self.velocity = {\n",
    "            'w': [np.zeros_like(w) for w in self.params['w']],\n",
    "            'b': [np.zeros_like(b) for b in self.params['b']]\n",
    "        }\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        \n",
    "    \n",
    "\n",
    "    def im2col(self, input_feat: np.ndarray, N, kh, kw, out_h, out_w, stride):\n",
    "        im2col_feat = []\n",
    "        for n in range(N):\n",
    "            for ih in range(out_h):\n",
    "                for iw in range(out_w):\n",
    "                    im2col_feat.append(input_feat[n, :, stride * ih:stride * ih + kh, stride * iw:stride * iw + kw])\n",
    "                    # each element -> (C, kh, kw)\n",
    "        # input_feat -> (N*out_h*out_w, C, kh, kw)\n",
    "\n",
    "        return np.array(im2col_feat).reshape(N * out_h * out_w, -1)\n",
    "\n",
    "    def convolution(self, input_feat: np.ndarray, filter: np.ndarray, kernel_size: tuple, stride=1, padding=0, bias=None):\n",
    "        '''\n",
    "        input_feat: (N, C, H, W)\n",
    "        filter: (out_C, in_C, kH, kw)\n",
    "        bias: (out_C, 1)\n",
    "        '''\n",
    "        N, C, H, W = input_feat.shape\n",
    "        kh, kw = kernel_size\n",
    "        out_h = int((H - kh + 2 * padding) // stride) + 1\n",
    "        out_w = int((W - kw + 2 * padding) // stride) + 1\n",
    "        out_c = filter.shape[1] \n",
    "        self.out_h = out_h\n",
    "        self.out_w = out_w\n",
    "        # 這裡定義的 filter dim 與原本相反，看 weight init 定義\n",
    "        \n",
    "        if padding:\n",
    "            input_feat = np.pad(input_feat, ((0, 0), (0, 0), (padding, padding), (padding, padding)), 'constant', constant_values=0)\n",
    "\n",
    "        im2col_feat = self.im2col(input_feat, N, kh, kw, out_h, out_w, stride)\n",
    "        # im2col -> (N*out_h*out_w, C*kh*kw)\n",
    "\n",
    "\n",
    "        # x @ w  \n",
    "        # x -> (N*out_h*out_w, C*kh*kw)\n",
    "        # w -> (C*kh*kw, out_c) note: 看 weight init 定義\n",
    "        if isinstance(bias, np.ndarray):\n",
    "            out_feat = (im2col_feat @ filter + bias).T\n",
    "        else:\n",
    "            out_feat = (im2col_feat @ filter ).T\n",
    "        # out_feat -> (out_c, N*out_h*out_w)\n",
    "        \n",
    "        # 直接 reshape 成 (N, out_c*out_h*out_w) 會產生順序錯亂\n",
    "        # 所以先將 dim2 的 out_c*out_h*out_w 拆開後再 permute\n",
    "        # out_feat -> (N, out_c, out_h, out_w)\n",
    "        return out_feat.reshape(out_c, N, out_h, out_w).transpose((1, 0, 2, 3))\n",
    "\n",
    "    def forward(self, params: dict, X) -> dict:\n",
    "        forward_saved = {'I': [], 'Y': []}\n",
    "        # X shape: (N, C, H, W)\n",
    "        \n",
    "        for idx, (w, b, act_func) in enumerate(zip(params['w'], params['b'], params['act_func'])):\n",
    "            if idx == 0:\n",
    "                I = self.convolution(X, w, self.kernel_size, self.stride, self.padding, b)\n",
    "            else:\n",
    "                # I = self.convolution(Y, w, self.kernel_size, self.stride, self.padding, b)\n",
    "                # flatten\n",
    "                Y = Y.reshape(X.shape[0], -1)\n",
    "                I = np.matmul(Y, w) + b\n",
    "            Y = act_func.forward(I)\n",
    "            forward_saved['I'].append(I)\n",
    "            forward_saved['Y'].append(Y)\n",
    "\n",
    "        return forward_saved\n",
    "    \n",
    "    def backward(self, params: dict, forward_val: dict, input_feat, label) -> dict:\n",
    "\n",
    "        params_delta = {'W': [], 'b': []}\n",
    "\n",
    "        bs = input_feat.shape[0]\n",
    "        norm_factor = 1 / bs\n",
    "        num_layers = len(params['w'])\n",
    "\n",
    "        for idx, (Y, I, w, act_func) in enumerate(zip(forward_val['Y'][::-1], \n",
    "                                                      forward_val['I'][::-1],\n",
    "                                                      params['w'][::-1], params['act_func'][::-1])):\n",
    "            if idx == 0:\n",
    "                delta = (Y - label) * act_func.backward(I)\n",
    "            else:\n",
    "                # delta[n+1] -> (bs, n_layer[n+1]), next_layer_w.T -> (n_layer[n+1], n_layer[n])\n",
    "                # delta = np.matmul(delta, next_layer_w.T) * act_func.backward(I)\n",
    "                # delta[n] -> (bs, n_layer[n])\n",
    "\n",
    "                # I -> (N, C, H, W) => (C, N*H*W) = (8, 2 * 26 * 26)\n",
    "                # np.matmul(delta, next_layer_w.T) -> (2, 8 * 26 * 26) => (8, 2 * 26 * 26)\n",
    "                delta = np.matmul(delta, next_layer_w.T) * act_func.backward(I.reshape(bs, -1))\n",
    "                # TODO\n",
    "            \n",
    "            if idx == num_layers - 1:\n",
    "                # prev_layer_Y -> (N*out_h*out_w, C*kh*kw) = (2 * 26 * 26, 1 * 3 * 3)\n",
    "                prev_layer_Y = self.im2col(input_feat, bs, kh, kw, self.out_h, self.out_w, self.stride)\n",
    "            else:\n",
    "                prev_layer_Y = forward_val['Y'][::-1][idx+1]\n",
    "         \n",
    "            # prev_layer_Y.T -> (n_layer[n-1], bs)\n",
    "            # delta -> (bs, n_layer[n])\n",
    "            dW = norm_factor * np.matmul(prev_layer_Y.T, delta)    \n",
    "            db = norm_factor * np.sum(delta, axis=0, keepdims=True)\n",
    "    \n",
    "            params_delta['W'].append(dW)\n",
    "            params_delta['b'].append(db)\n",
    "            next_layer_w = w\n",
    "\n",
    "        return params_delta\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "(2, 8, 26, 26)\n"
     ]
    }
   ],
   "source": [
    "input_feat = np.random.randint(0, 255, size=(2, 1, 28, 28)) / 255\n",
    "myconv = MyModel(1, 8, (3, 3), 1, 0)\n",
    "\n",
    "my_res_trans = myconv.forward(myconv.params, input_feat)['I'][0]\n",
    "my_res = convolution(input_feat, myconv.params['w'][0].T, 3, 3, 1, 0, myconv.params['b'][0].T)\n",
    "\n",
    "print(np.allclose(my_res_trans, my_res))\n",
    "\n",
    "print(my_res_trans.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myconv.forward(myconv.params, input_feat)['Y'][1]\n",
    "# my_res_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.prod((1, 3, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_mlp import MLP, CrossEntropyLoss\n",
    "from my_nn_lib import ReLU, Softmax, LeckyReLU, Linear\n",
    "\n",
    "class MyModel(MLP):\n",
    "    def __init__(self, layer_list):\n",
    "        self.layers = layer_list\n",
    "\n",
    "    def forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X)\n",
    "        return X\n",
    "    \n",
    "    def backward(self, X, label):\n",
    "        N = X.shape[0]\n",
    "        norm_factor = 1 / N\n",
    "        idx_maxlayer = len(self.layers)-1\n",
    "        dLdZ = label\n",
    "\n",
    "        for idx in range(idx_maxlayer, -1, -1):\n",
    "            # 倒敘遍歷\n",
    "\n",
    "            # 前一層的輸出(當前層的輸入X)\n",
    "            # Y_prev = self.layers[idx-1].activations if idx != 0 else X\n",
    "\n",
    "            # if idx == idx_maxlayer:\n",
    "            #     dLdZ = self.layers[idx].backward(dLdZ=None, Y_prev=Y_prev, is_output=True, label=label)\n",
    "            # else:\n",
    "            dLdZ = self.layers[idx].backward(delta=dLdZ)\n",
    "            \n",
    "            # activation func & flatten 不用更新權重\n",
    "            \n",
    "        \n",
    "        # return params_delta\n",
    "    \n",
    "    def update_params(self, opt_params):\n",
    "        for layer in self.layers:\n",
    "            layer.update_params(opt_params)\n",
    "        \n",
    "    def get_pred(self, X, with_onehot=False):\n",
    "        pred = self.forward(X)\n",
    "        if with_onehot:\n",
    "            return pred\n",
    "        return np.argmax(pred, axis=1)\n",
    "\n",
    "    def train(self, X_train, Y_train, X_val, Y_val, loss_func, hyper_params: dict, show_plot=False):\n",
    "        # X_train -> (n_samples, n_features)\n",
    "        # Y_train -> (n_samples, n_classes) one-hot \n",
    "        \n",
    "        # params = self.weight_init(self.params_set_list)\n",
    "\n",
    "        n_samples = X_train.shape[0]\n",
    "\n",
    "        # 將 train data 打包成 batch\n",
    "        X_batch_all, Y_batch_all = self.pack_to_batch(X_train, Y_train, hyper_params['batch_size'], n_samples)\n",
    "        \n",
    "        train_loss_arr = []\n",
    "        val_loss_arr = []\n",
    "        \n",
    "        val_acc_arr = []\n",
    "\n",
    "        for i in range(hyper_params['epoch']):\n",
    "            loss = 0\n",
    "            for X_batch, Y_batch in zip(X_batch_all, Y_batch_all):\n",
    "                # 單個 batch 訓練過程\n",
    "                # 1. 前向傳播\n",
    "                # 2. 反向傳播\n",
    "                # 3. 更新權重   \n",
    "                self.forward(X_batch)\n",
    "                self.backward(X_batch, Y_batch)\n",
    "                self.update_params({'lr': hyper_params['lr'], 'alpha': hyper_params['alpha']})\n",
    "                loss += loss_func.cal_loss(self.get_pred(X_batch, with_onehot=True), Y_batch)\n",
    "              \n",
    "            # print(\"Epoch: \", i)\n",
    "            # print('Loss:', round(loss, 2))\n",
    "\n",
    "            predictions = self.get_pred(X_val)\n",
    "            # print('Val Acc:', round(get_accuracy(predictions, Y_val), 2))\n",
    "            \n",
    "            train_loss_arr.append(loss / n_samples)\n",
    "\n",
    "            # 取 output layer 經過 activation function 的結果為 prediction\n",
    "            val_loss_arr.append(loss_func.cal_loss(self.get_pred(X_val, with_onehot=True), Y_val) / len(X_val))\n",
    "            val_acc_arr.append(self.calculate_acc(predictions, Y_val))\n",
    "\n",
    "        if show_plot:\n",
    "            self.plot_loss_acc(train_loss_arr, val_loss_arr, val_acc_arr)\n",
    "\n",
    "        return train_loss_arr, val_loss_arr, val_acc_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "backward() got an unexpected keyword argument 'dLdZ'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 89\u001b[0m\n\u001b[1;32m     80\u001b[0m model \u001b[38;5;241m=\u001b[39m MyModel([Linear(\u001b[38;5;241m11\u001b[39m, \u001b[38;5;241m32\u001b[39m), \n\u001b[1;32m     81\u001b[0m                  ReLU(), \n\u001b[1;32m     82\u001b[0m                  Linear(\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m8\u001b[39m), \n\u001b[1;32m     83\u001b[0m                  ReLU(), \n\u001b[1;32m     84\u001b[0m                  Linear(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m6\u001b[39m), \n\u001b[1;32m     85\u001b[0m                  Softmax()])\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# model = MyModel([Linear(4, 3), Softmax()])\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyper_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_plot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# mlp.kfold(X_data, Y_data, FOLD, SquareLoss, hyper_params)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 69\u001b[0m, in \u001b[0;36mMyModel.train\u001b[0;34m(self, X_train, Y_train, X_val, Y_val, loss_func, hyper_params, show_plot)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X_batch, Y_batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(X_batch_all, Y_batch_all):\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# 單個 batch 訓練過程\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# 1. 前向傳播\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m# 2. 反向傳播\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m# 3. 更新權重   \u001b[39;00m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(X_batch)\n\u001b[0;32m---> 69\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_params({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m: hyper_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m: hyper_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m]})\n\u001b[1;32m     71\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_func\u001b[38;5;241m.\u001b[39mcal_loss(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_pred(X_batch, with_onehot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), Y_batch)\n",
      "Cell \u001b[0;32mIn[1], line 28\u001b[0m, in \u001b[0;36mMyModel.backward\u001b[0;34m(self, X, label)\u001b[0m\n\u001b[1;32m     17\u001b[0m dLdZ \u001b[38;5;241m=\u001b[39m label\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(idx_maxlayer, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# 倒敘遍歷\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m#     dLdZ = self.layers[idx].backward(dLdZ=None, Y_prev=Y_prev, is_output=True, label=label)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m     dLdZ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdLdZ\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdLdZ\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: backward() got an unexpected keyword argument 'dLdZ'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 固定隨機種子，確保同參數下每次結果都相同\n",
    "np.random.seed(1)\n",
    "\n",
    "def wine():\n",
    "    with open('data/winequality-red.csv') as f:\n",
    "        # 跳過 first row (標籤名稱)\n",
    "        data = f.readlines()[1:]\n",
    "\n",
    "    data = [line.strip().split(',') for line in data]\n",
    "\n",
    "    data = np.array(data)\n",
    "    labels = []\n",
    "\n",
    "    # 最後一個 column 為 label\n",
    "    classes = np.unique(data[:, -1])\n",
    "\n",
    "    # 將 label 做 one-hot encoding\n",
    "    for d in data:\n",
    "        for cls in classes:\n",
    "            if d[-1] == cls:\n",
    "                one_hot = np.zeros(len(classes))\n",
    "                one_hot[classes.tolist().index(cls)] = 1\n",
    "                labels.append(one_hot)\n",
    "    return data, labels\n",
    "\n",
    "def iris():\n",
    "    with open('data/Iris.csv') as f:\n",
    "    # 跳過 first row (標籤名稱)\n",
    "        data = f.readlines()[1:]\n",
    "\n",
    "    data = [line.strip().split(',')[1:] for line in data]\n",
    "    data = np.array(data)\n",
    "    labels = []\n",
    "\n",
    "    classes = np.unique(data[:, 4])\n",
    "\n",
    "    # 將 label 做 one-hot encoding\n",
    "    for d in data:\n",
    "        for cls in classes:\n",
    "            if d[4] == cls:\n",
    "                one_hot = np.zeros(len(classes))\n",
    "                one_hot[classes.tolist().index(cls)] = 1\n",
    "                labels.append(one_hot)\n",
    "    return data, labels\n",
    "\n",
    "# data, labels = iris()\n",
    "data, labels = wine()\n",
    "# 將 input features 與 labels 從原始資料中分離\n",
    "inputs = data[:, :-1].astype(np.float32)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# 對 input features 做標準化\n",
    "inputs = (inputs - np.mean(inputs, axis=0)) / np.std(inputs, axis=0)\n",
    "\n",
    "# 打亂數據\n",
    "idx = np.random.permutation(len(inputs))\n",
    "\n",
    "X_data = inputs[idx]\n",
    "Y_data = labels[idx]\n",
    "\n",
    "# 設定 train set 和 val set 的比例 (80% train, 20% val)\n",
    "train_size = int(len(X_data) * 0.80)\n",
    "\n",
    "\n",
    "X_train, Y_train = X_data[:train_size], Y_data[:train_size]\n",
    "X_val, Y_val = X_data[train_size:], Y_data[train_size:]\n",
    "\n",
    "hyper_params = {\n",
    "    'lr': 0.01,\n",
    "    'epoch': 50,\n",
    "    'batch_size': 16,\n",
    "    'alpha': 0.9\n",
    "}\n",
    "\n",
    "FOLD = 5\n",
    "\n",
    "\n",
    "model = MyModel([Linear(11, 32), \n",
    "                 ReLU(), \n",
    "                 Linear(32, 8), \n",
    "                 ReLU(), \n",
    "                 Linear(8, 6), \n",
    "                 Softmax()])\n",
    "\n",
    "# model = MyModel([Linear(4, 3), Softmax()])\n",
    "\n",
    "params = model.train(X_train, Y_train, X_val, Y_val, CrossEntropyLoss, hyper_params, show_plot=True)\n",
    "# mlp.kfold(X_data, Y_data, FOLD, SquareLoss, hyper_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.56901584, -0.33591916,  0.32947425],\n",
       "       [-6.07379117, -0.09827952,  0.09863074],\n",
       "       [ 6.70993082,  0.16028335,  0.27324872],\n",
       "       [ 6.01571516, -0.03469986, -0.23115943]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2428 1562 1441]\n",
      " [1636  983 1396]\n",
      " [1096  796 1088]\n",
      " [1153  864  932]\n",
      " [1306  845 1371]] [[2428 1562 1441]\n",
      " [1636  983 1396]\n",
      " [1096  796 1088]\n",
      " [1153  864  932]\n",
      " [1306  845 1371]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.random.randint(0, 255, size=(5, 5))\n",
    "b = np.random.randint(0, 5, size=(5, 3))\n",
    "\n",
    "print(a @ b, np.dot(a, b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Tensor(np.ndarray):\n",
    "    def __new__(cls, input_array, requires_grad=False):\n",
    "        # 建立 ndarray 的子類\n",
    "        obj = np.asarray(input_array).view(cls)\n",
    "        obj.requires_grad = requires_grad\n",
    "        obj.grad = None  # 儲存梯度\n",
    "        obj._grad_fn = None  # 計算梯度的函數\n",
    "        return obj\n",
    "\n",
    "    def __array_finalize__(self, obj):\n",
    "        # 當新 Tensor 被創建時，繼承屬性\n",
    "        if obj is None: return\n",
    "        self.requires_grad = getattr(obj, 'requires_grad', False)\n",
    "        self.grad = getattr(obj, 'grad', None)\n",
    "        self._grad_fn = getattr(obj, '_grad_fn', None)\n",
    "\n",
    "    def backward(self, grad_output=None):\n",
    "        if not self.requires_grad:\n",
    "            raise RuntimeError(\"This tensor does not require gradients.\")\n",
    "        \n",
    "        if grad_output is None:\n",
    "            grad_output = np.ones_like(self)\n",
    "        \n",
    "        if self.grad is None:\n",
    "            self.grad = grad_output\n",
    "        else:\n",
    "            self.grad += grad_output\n",
    "\n",
    "        if self._grad_fn:\n",
    "            self._grad_fn(grad_output)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Tensor({super().__repr__()}, requires_grad={self.requires_grad})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(Tensor([[1, 2],\n",
       "        [3, 4]]), requires_grad=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.array([[1, 2], [3, 4]])\n",
    "a = Tensor(b)\n",
    "a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.Tensor"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparsedrive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
